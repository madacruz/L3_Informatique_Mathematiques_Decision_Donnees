{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction d'un système de recommandation\n",
    "\n",
    "Nous avons décidé d'orienter notre projet sur la recommendation de films.\n",
    "En effet durant ce confinement, nous avons eu le temps de visionner beaucoup de films,\n",
    "mais nous nous sommes rendus compte que nous passions quasiment autant de temps\n",
    "à choisir le film qu'à le regarder. D'où la nécessité de créer un système de re-\n",
    "commendations afin d'optimiser notre temps de visionnage.\n",
    "Nous avons chercher une base de données assez exploitable afin de mener à bien\n",
    "notre projet. Nous nous sommes basés sur la base de données de 'The Movies Dataset'.\n",
    "\n",
    "[sommaire](#sommaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reste à faire\n",
    "\n",
    "- [x] approche linéaire \n",
    "- [x] class hybride\n",
    "- [x] CV grid search pour model-based hyperparamètres\n",
    "- [ ] comparer score pour les prédicteurs\n",
    "- [ ] comparer score pour les predicteurs hybride (devrait être moindre)\n",
    "- [ ] documentation docstring\n",
    "- [ ] documentation markdown\n",
    "\n",
    "# À dire dans rapport pdf\n",
    "\n",
    "- utilisation des magic command timeit, lprun pour optimiser le code, trop lent au début, on a réussi à accélérer tout ça en utilisant des numpy array quand c'était possible et en indexant ratings avec userId et movieId pour des accès plus direct (note d'Anita : est ce qu'on a bien fait ça tout le temps ? moi oui mais j'ai pas regardé pour vos fonctions)\n",
    "- présentation succinte des différentes méthodes implémentée\n",
    "- que la plupart du temps, on donne les hyperparametres en arguments mais qu'on leur donne nos valeurs pref apr défaut. Comme ça, ça peut tourner avec nos choix d'hyperparam, mais que l'utilisateur peut quand même avoir la main dessus\n",
    "- sources de nos inspi\n",
    "- pourquoi pas de k-mean ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation des packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "\n",
    "from ast import literal_eval\n",
    "from itertools import product\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement et nettoyage des données\n",
    "\n",
    "<span style=\"color:blue\"> description des différentes tables </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "movies = pd.read_csv(\"movies_metadata.csv\")\n",
    "ratings = pd.read_csv(\"ratings_small.csv\")\n",
    "keywords = pd.read_csv(\"keywords.csv\")\n",
    "credits = pd.read_csv(\"tmdb_5000_credits.csv\")\n",
    "link = pd.read_csv(\"links_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>[{'id': 931, 'name': 'jealousy'}, {'id': 4290,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>[{'id': 10090, 'name': 'board game'}, {'id': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>[{'id': 1495, 'name': 'fishing'}, {'id': 12392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>[{'id': 818, 'name': 'based on novel'}, {'id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>[{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           keywords\n",
       "0    862  [{'id': 931, 'name': 'jealousy'}, {'id': 4290,...\n",
       "1   8844  [{'id': 10090, 'name': 'board game'}, {'id': 1...\n",
       "2  15602  [{'id': 1495, 'name': 'fishing'}, {'id': 12392...\n",
       "3  31357  [{'id': 818, 'name': 'based on novel'}, {'id':...\n",
       "4  11862  [{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...</td>\n",
       "      <td>[{\"credit_id\": \"52fe48009251416c750aca23\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>[{\"cast_id\": 4, \"character\": \"Captain Jack Spa...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206647</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>[{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...</td>\n",
       "      <td>[{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>[{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49529</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>[{\"cast_id\": 5, \"character\": \"John Carter\", \"c...</td>\n",
       "      <td>[{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                     title  \\\n",
       "0     19995                                    Avatar   \n",
       "1       285  Pirates of the Caribbean: At World's End   \n",
       "2    206647                                   Spectre   \n",
       "3     49026                     The Dark Knight Rises   \n",
       "4     49529                               John Carter   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...   \n",
       "1  [{\"cast_id\": 4, \"character\": \"Captain Jack Spa...   \n",
       "2  [{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...   \n",
       "3  [{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...   \n",
       "4  [{\"cast_id\": 5, \"character\": \"John Carter\", \"c...   \n",
       "\n",
       "                                                crew  \n",
       "0  [{\"credit_id\": \"52fe48009251416c750aca23\", \"de...  \n",
       "1  [{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...  \n",
       "2  [{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...  \n",
       "3  [{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...  \n",
       "4  [{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correct_id(word):\n",
    "    if not isinstance(word, str) or re.fullmatch(r'[0-9]+', word):\n",
    "        return word\n",
    "    return \"wrong_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_genre(l):\n",
    "    if len(l) <= 0 :\n",
    "        return []\n",
    "    if isinstance(l[0], dict):\n",
    "        return [d['name'] for d in l]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> explication du nettoyage effectué </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not re-run !\n",
    "\n",
    "movies = movies.drop_duplicates('id')\n",
    "keywords = keywords.drop_duplicates('id')\n",
    "credits = credits.drop_duplicates('movie_id')\n",
    "\n",
    "movies.id = movies.id.apply(filter_correct_id)\n",
    "movies = movies[movies.id != \"wrong_id\"]\n",
    "movies.id = movies.id.astype('int64')\n",
    "movies[movies['vote_count'].notnull()]['vote_count'].astype('int64', copy=False)\n",
    "movies[movies['vote_average'].notnull()]['vote_average'].astype('int64', copy=False)\n",
    "\n",
    "ratings = ratings.drop(columns=['timestamp'])\n",
    "\n",
    "movies = movies.rename(columns={'id' : 'tmdbId'})\n",
    "keywords = keywords.rename(columns={'id' : 'tmdbId'})\n",
    "credits = credits.rename(columns={'movie_id' : 'tmdbId'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire <a id='sommaire'></a>\n",
    "\n",
    "Nous avons différenciés deux types de systèmes de recommendations. \n",
    "\n",
    "- Les **Recommender** recommendent à un utilisateur des films qu'il sera susceptible d'aimer, sans explicitement prédire la note que l'utilisateur donnera à ses films. Nous en avons implémenté deux types:\n",
    "\n",
    "    1. un système utilisant la popularité des films dans une catégorie donnée [popularity-based](#popularity)\n",
    "    2. un système utilisant les méta-informations des films [clustering](#clustering)\n",
    "\n",
    "\n",
    "- Les **Predictor** prédisent les notes que des utilisateurs donneront à des films. Nous en avons implémenté deux types :\n",
    "    3. un système basé sur les données connues (user- et item-based présenté en cours) [memory-based](#memory)\n",
    "    4. un système basé sur un modèle [model-based](#model)\n",
    "\n",
    "\n",
    "- Les systèmes **Hybride** font le mélange des deux. Ils recommendent à un utilisateur des films qu'il sera susceptible d'aimer en utilisant des prédictions de notes. Nous avons implémenté une classe qui prend en argument un Recommender et un Predictor:\n",
    "    5. [hybride](#hybride)\n",
    "    \n",
    "\n",
    "- Compaison critique des différentes méthodes : [score](#score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Films les plus populaires par genre <a id='popularity'></a>\n",
    "\n",
    "[retour au sommaire](#sommaire)\n",
    "\n",
    "Une méthode simple pour recommander des films à un utilisateur, est de lui présenter la liste des films les plus populaires appartenant à ses genres préférées. Ainsi il faudra dans un premier temps identifier ses genres préférés, puis établir la liste des films les plus populaires qui figurent parmi ces genres.\n",
    "\n",
    "Pour cette méthode, nous allons utiliser les données contenues dans les colonnes `vote_average` et `vote_count`. En effet, il y a plus de votes récoltés dans cette table quand dans `ratings`. Les utilisateurs ont utilisé une notation allant de 0 à 10.\n",
    "\n",
    "L'initialisation d'un objet de `PopularityRecommender` traite les données entrée pour rendre la recommendation plus efficace. La dataframe `dfm` de schéma *dfm(movieId, title, vote_average, vote_count)* contient la liste des films identifiés par leur `movieId`. Pour pouvoir sélectionner les films par genre, et parce qu'un film peut appartenir à plusieurs genres, nous utlisons une table `sgr` dans laquelle chaque ligne n'indique qu'un seul genre du film. Indexer ces deux tables par l'attribut `movieId` nous permet d'accélerer la sélection des lignes puisqu'elle ne se fait presque que par `movieId`. Enfin, la table `dfm` contient les notes des utilisateurs et des films. Elle est utilisée pour déterminer les genres préféres d'un user donné : la méthode `pref_genres` selectionne les genres des 3 films préférés de l'utilisateurs. \n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> selection des genres peut être à faire de manière plus accurate ? genre donner plus d'importance à un certain genre ? </span>\n",
    "\n",
    "<span style=\"color:blue\"> utilisation d'une fonction extérieure à la classe : même problème avec memory et les fct de corrélations </span>\n",
    "\n",
    "\n",
    "Pour évaluer la popularitée d'un film au sein d'un catégorie de films, nous utilisons la formule de *weighted rating* utilisée par le site TMDB : \n",
    "$$\n",
    "WR = \\frac{v}{v + m} R + \\frac{m}{v +m} C\n",
    "$$\n",
    "\n",
    "où \n",
    "- $R$ est la note moyenne du film (vote_average) ;\n",
    "\n",
    "- $v$ est le nombre de notes que le film a reçu (vote_count) ; \n",
    "\n",
    "- $m$ est le nombre minimum de votes qu'un film doit recevoir pour pouvoir figurer sur la liste ;\n",
    "\n",
    "- $C$ est la note moyenne donnée dans toute l'étude.\n",
    "\n",
    "\n",
    "Si $R$, $v$ et $C$ se calculent à partir des données, il nous faut choisir le seuil $m$. Nous allons considérer qu'un film doit avoir eu plus de votes qu'au moins 80% des films pour pouvoir aparaître dans le top du genre. Ce paramètre permet de ne considérer que des films qui ont été vu par une majorité de personnes et qui peuvent être donc considérés comme populaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "    def __init__(self):\n",
    "        self.dfr = None\n",
    "        self.dfm = None\n",
    "        self.sgr = None\n",
    "    \n",
    "    def fit(self, movies, ratings, link) :\n",
    "        self.dfr = ratings[['userId', 'movieId', 'rating']].set_index(['userId', 'movieId'])\n",
    "        \n",
    "        # construction de la table des films identifiés par leur movieId\n",
    "        self.dfm = movies[['tmdbId', 'title', 'genres', 'vote_average', 'vote_count']]\n",
    "        self.dfm = link.merge(self.dfm, left_on='tmdbId', right_on='tmdbId').drop(columns=['tmdbId', 'imdbId'])\n",
    "        self.dfm.set_index('movieId', inplace=True)\n",
    "        \n",
    "        # transformer la description JSON de genres en liste de string plus simple à manipuler\n",
    "        self.dfm['genres'] = self.dfm['genres']\\\n",
    "                    .apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\\\n",
    "                    .apply(simplify_genre)\n",
    "\n",
    "        # construction d'une série dans laquelle les lignes ne contiennent qu'un seul genre\n",
    "        self.sgr = self.dfm.apply(lambda x: pd.Series(x['genres'], dtype='str'),axis=1).stack().reset_index(level=1, drop=True)\n",
    "        self.sgr.name = 'genre'\n",
    "    \n",
    "    def recommend(self, uid, dfr, n=20):\n",
    "        '''\n",
    "        :param: uid l'identifiant de l'utilisateur\n",
    "                n le nombre de films à recommender\n",
    "        :return: une dataframe contenant les n films les plus populaires appartenant aux genres préférés de l'user uid\n",
    "        '''\n",
    "        assert self.dfr is not None, \"This PopularityRecommender instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        \n",
    "        # récupérer les films les plus populaires des genres préférés de uid\n",
    "        chart = self._best_(self.pref_genres(uid), n*10)\n",
    "        \n",
    "        # ne retenir que les films non encore visionnés\n",
    "        watched_movies = self.dfr.loc[uid, :].index.unique()\n",
    "        watched_movies = chart.index.isin(watched_movies)\n",
    "        chart = chart.loc[~watched_movies]\n",
    "\n",
    "\n",
    "        return chart.head(n).index if len(chart) > n else chart.index\n",
    "    \n",
    "\n",
    "    def pref_genres(self, uid):\n",
    "        '''\n",
    "        :param: uid l'identifiant de l'utilisateur\n",
    "        :return: une liste des genres des 3 films préférés de l'user uid\n",
    "        '''\n",
    "        rats = self.dfr.loc[uid, :].sort_values(by='rating', ascending=False)\n",
    "        pref = rats.head(3).index if rats.shape[0] > 5 else rats.index\n",
    "        genres = []\n",
    "        for g in self.dfm.loc[pref].genres :\n",
    "            genres = list(set(genres) | set(g))\n",
    "        return genres\n",
    "\n",
    "    def _best_(self, genres, k=200):\n",
    "        '''\n",
    "        :param: genres une liste de genres sous forme de str\n",
    "                k le nombre de films à retenir\n",
    "        :return: les k films les plus populaires appartenant à au moins un genre dans la liste genres\n",
    "        '''\n",
    "        # select movies that are in genres\n",
    "        dfg = self.sgr[self.sgr.isin(genres)]\n",
    "        dfg = self.dfm.loc[dfg.index.drop_duplicates()] \n",
    "        # need to drop duplicates : a movie can be selected because 2 or more of its genres are ok\n",
    "\n",
    "        m = dfg.vote_count.quantile(0.8)\n",
    "        \n",
    "        # sort movies by decresing weighted rating\n",
    "        top = dfg[(dfg['vote_count'] >= m)]\n",
    "        top['wr'] = self.weighted_rating(top[['vote_count', 'vote_average']].to_numpy(), m)\n",
    "        top.sort_values('wr', ascending=False, inplace=True)\n",
    "\n",
    "        # drop unnecessary columns\n",
    "        top.drop(columns=['vote_count', 'vote_average'], inplace=True)\n",
    "\n",
    "        return top.head(k) if len(top) > k else top\n",
    "    \n",
    "    def weighted_rating(self, x, m):\n",
    "        '''\n",
    "        :param: x une matrice numpy de format suivant\n",
    "                    chaque ligne correspond à un films\n",
    "                    la première colonne x[: 0] contient le nombre de votes reçus par le films\n",
    "                    la deuxième colonne x[: 1] content la note moyenne de chaque film\n",
    "                m le nombre minimum de votes qu'un film doit recevoir pour pouvoir figurer sur la liste\n",
    "        :return: un numpy array contenant la note pondérée que chaque film\n",
    "        '''\n",
    "        v = x[:, 0] # liste du nombre de vote\n",
    "        R = x[:, 1] # liste des notes moyennes\n",
    "        C = sum(np.multiply(v, R)) / sum(v) # note moyenne attribuée\n",
    "        return np.multiply(v/(v+m), R) + np.multiply(m/(m+v), C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering des films<a id='clustering'></a>\n",
    "\n",
    "[retour au sommaire](#sommaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de la base de données et réduction de la matrice aux caractéristiques interéssantes\n",
    "\n",
    "Suppression des id incorrects, des valeurs abérrantes, des lignes avec NaN, et modification des valeurs pour les rendre plus faciles à traiter.\n",
    "\n",
    "On sélectionne les attributs de films qui semblent pertinents pour différencier les films sur leur contenu.\n",
    "Ces choix sont arbitraires et on pourra être amenés à réfléchir dessus et à les modifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne voulons garder que les films ayant reçu une note. Cela est une manière de ne garder qu'un nombre limité de films (il est très compliqué pour nous d'effectuer des calculs pour 45 000 films). De plus le clustering est intéressant pour renforcer la recommendation \"user-based\". On ne garde donc que les films ayant été notés par les utilisateurs. Ensuite on rajoute l'attribut keywords aux films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_cluster = movies.join(link.set_index('tmdbId'), on='tmdbId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_cluster = dfm_cluster.merge(ratings.drop_duplicates('movieId'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_cluster = dfm_cluster.join(keywords.set_index('tmdbId'), on='tmdbId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>release_date</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>original_language</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>[{'id': 931, 'name': 'jealousy'}, {'id': 4290,...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>en</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>[{'id': 10090, 'name': 'board game'}, {'id': 1...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>en</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>[{'id': 1495, 'name': 'fishing'}, {'id': 12392...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>en</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>[{'id': 818, 'name': 'based on novel'}, {'id':...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>en</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>[{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>en</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title  \\\n",
       "movieId                                \n",
       "1                          Toy Story   \n",
       "2                            Jumanji   \n",
       "3                   Grumpier Old Men   \n",
       "4                  Waiting to Exhale   \n",
       "5        Father of the Bride Part II   \n",
       "\n",
       "                                                    genres  \\\n",
       "movieId                                                      \n",
       "1        [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "2        [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "3        [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "4        [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "5                           [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                                                  keywords release_date  \\\n",
       "movieId                                                                   \n",
       "1        [{'id': 931, 'name': 'jealousy'}, {'id': 4290,...   1995-10-30   \n",
       "2        [{'id': 10090, 'name': 'board game'}, {'id': 1...   1995-12-15   \n",
       "3        [{'id': 1495, 'name': 'fishing'}, {'id': 12392...   1995-12-22   \n",
       "4        [{'id': 818, 'name': 'based on novel'}, {'id':...   1995-12-22   \n",
       "5        [{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...   1995-02-10   \n",
       "\n",
       "                                      production_countries original_language  \\\n",
       "movieId                                                                        \n",
       "1        [{'iso_3166_1': 'US', 'name': 'United States o...                en   \n",
       "2        [{'iso_3166_1': 'US', 'name': 'United States o...                en   \n",
       "3        [{'iso_3166_1': 'US', 'name': 'United States o...                en   \n",
       "4        [{'iso_3166_1': 'US', 'name': 'United States o...                en   \n",
       "5        [{'iso_3166_1': 'US', 'name': 'United States o...                en   \n",
       "\n",
       "         runtime  \n",
       "movieId           \n",
       "1           81.0  \n",
       "2          104.0  \n",
       "3          101.0  \n",
       "4          127.0  \n",
       "5          106.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_id = dfm_cluster[['tmdbId','movieId','title']]\n",
    "cluster_features = dfm_cluster[['title', 'genres', 'keywords', 'release_date', 'production_countries', 'original_language', 'runtime']]\n",
    "cluster_features.index = dfm_cluster.movieId.apply(lambda x: str(x))\n",
    "cluster_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11a2dec50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cluster_features.runtime.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit de ne retenir que les films d'une durée comprise entre 40 minutes et 4 heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_runtime(dfm, inf=40, sup=240):\n",
    "    dfm = dfm[dfm.runtime >= inf]\n",
    "    dfm = dfm[dfm.runtime <= sup]\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = clean_runtime(cluster_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde la proportion de films pour lesquels certains champs n'ont pas été renseignés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de films retenus dans cluster_features :  8897\n",
      "Parmi ces films :\n",
      "30 n'ont pas de genres\n",
      "718 n'ont pas de keywords\n",
      "270 n'ont pas de production_countries\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de films retenus dans cluster_features : \", len(cluster_features))\n",
    "print(\"Parmi ces films :\")\n",
    "print(len(cluster_features[cluster_features.genres == \"[]\"]), \"n'ont pas de genres\")\n",
    "print(len(cluster_features[cluster_features.keywords == \"[]\"]), \"n'ont pas de keywords\")\n",
    "print(len(cluster_features[cluster_features.production_countries == \"[]\"]), \"n'ont pas de production_countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'une petite proportion, on peut donc retirer ces films problématiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(dfm):\n",
    "    \n",
    "    '''This function takes a movie DataFrame with genres, keywords and production_countries\n",
    "    attributes, and it removes the rows for which there is no information on these features,\n",
    "    i.e. when one or more attributes of the movie Series equals the string of an empty list.'''\n",
    "    \n",
    "    dfm = dfm[dfm.genres != \"[]\"]\n",
    "    dfm = dfm[dfm.keywords != \"[]\"]\n",
    "    dfm = dfm[dfm.production_countries != \"[]\"]\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = drop_missing_values(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de films dans cluster_features :  8037\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de films dans cluster_features : \", len(cluster_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant se concentrer sur le traitement des données de chacune des colonnes. Il faut les simplifier au maximum pour rendre possible la comparaison de films basée sur ces attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_genres(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'genres'.\n",
    "    It turns the column genres from a string that contains a dictionnary into an int list of genres id.'''\n",
    "    \n",
    "    def genres_to_id(gen):\n",
    "        if isinstance(gen, str):\n",
    "            pattern = re.compile(r\"'id': [0-9]*\")\n",
    "            return np.array([int(w[6:]) for w in pattern.findall(gen)])\n",
    "        return gen\n",
    "    \n",
    "    dfm.genres = dfm.genres.apply(genres_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_genres(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_keywords(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'keywords'.\n",
    "    It turns the column keywords from a string that contains a dictionnary into an int list of keywords id.'''\n",
    "    \n",
    "    def keywords_to_id(kw):\n",
    "        if isinstance(kw, str):\n",
    "            pattern = re.compile(r\"'id': [0-9]*\")\n",
    "            return np.array([int(w[6:]) for w in pattern.findall(kw)])\n",
    "        return kw\n",
    "    \n",
    "    dfm.keywords = dfm.keywords.apply(keywords_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_keywords(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_date(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'release_date'.\n",
    "    It turns the column release_date from a string date into an int being the year the film was released.'''\n",
    "    \n",
    "    def date_to_int(date):\n",
    "        if isinstance(date, str):\n",
    "            return int(date[:4])\n",
    "        return date\n",
    "    \n",
    "    dfm.release_date = dfm.release_date.apply(date_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_date(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_countries(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'production_countries'.\n",
    "    It turns the column production_countries from a string that contains a dictionnary into an int list of keywords id.'''\n",
    "    \n",
    "    def simplify(country):\n",
    "        if isinstance(country, str):\n",
    "            pattern = re.compile(r\"'iso_3166_1': ...\")\n",
    "            return [w[15:] for w in pattern.findall(country)]\n",
    "        return country\n",
    "    \n",
    "    dfm.production_countries = dfm.production_countries.apply(simplify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_countries(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>release_date</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>original_language</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>[16, 35, 10751]</td>\n",
       "      <td>[931, 4290, 5202, 6054, 9713, 9823, 165503, 17...</td>\n",
       "      <td>1995</td>\n",
       "      <td>[US]</td>\n",
       "      <td>en</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>[12, 14, 10751]</td>\n",
       "      <td>[10090, 10941, 15101, 33467, 158086, 158091]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[US]</td>\n",
       "      <td>en</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>[10749, 35]</td>\n",
       "      <td>[1495, 12392, 179431, 208510]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[US]</td>\n",
       "      <td>en</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>[35, 18, 10749]</td>\n",
       "      <td>[818, 10131, 14768, 15160, 33455]</td>\n",
       "      <td>1995</td>\n",
       "      <td>[US]</td>\n",
       "      <td>en</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>[35]</td>\n",
       "      <td>[1009, 1599, 2246, 4995, 5600, 10707, 13149, 3...</td>\n",
       "      <td>1995</td>\n",
       "      <td>[US]</td>\n",
       "      <td>en</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title           genres  \\\n",
       "movieId                                                 \n",
       "1                          Toy Story  [16, 35, 10751]   \n",
       "2                            Jumanji  [12, 14, 10751]   \n",
       "3                   Grumpier Old Men      [10749, 35]   \n",
       "4                  Waiting to Exhale  [35, 18, 10749]   \n",
       "5        Father of the Bride Part II             [35]   \n",
       "\n",
       "                                                  keywords  release_date  \\\n",
       "movieId                                                                    \n",
       "1        [931, 4290, 5202, 6054, 9713, 9823, 165503, 17...          1995   \n",
       "2             [10090, 10941, 15101, 33467, 158086, 158091]          1995   \n",
       "3                            [1495, 12392, 179431, 208510]          1995   \n",
       "4                        [818, 10131, 14768, 15160, 33455]          1995   \n",
       "5        [1009, 1599, 2246, 4995, 5600, 10707, 13149, 3...          1995   \n",
       "\n",
       "        production_countries original_language  runtime  \n",
       "movieId                                                  \n",
       "1                       [US]                en     81.0  \n",
       "2                       [US]                en    104.0  \n",
       "3                       [US]                en    101.0  \n",
       "4                       [US]                en    127.0  \n",
       "5                       [US]                en    106.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition d'une distance sur les films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va réaliser plus bas un hierarchical agglomerative clustering. Le principe est simple : on commence avec n clusters de 1 film, puis on fusionne à chaque itération les 2 clusters les plus proches, jusqu'à n'avoir plus d'un seul cluster contenant tous les films. Cela requiert une distance sur les films. C'est ce qu'on va construire ici. La tâche n'est pas simple : chaque film a été réduit à 6 attributs, et il faut aggréger ces 6 attributs pour déterminer à quel point 2 films sont similaires. On peut choisir d'accorder un poids différent à chacun des critères en fonction de leur importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit ici des variables globales qui seront utilisées plus loin dans la fonction movie_distance\n",
    "MAX_YEAR_DIFFERENCE = max(cluster_features.release_date) - min(cluster_features.release_date)\n",
    "MAX_RUNTIME_DIFFERENCE = max(cluster_features.runtime) - min(cluster_features.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction movie_distance calcule une distance entre 2 films. Plus cette valeur est proches de 0 et plus les films sont similaires. Plus la valeur est grande et plus ils sont différents. IMPORTANT : la built-in magic command %lprun nous a permis d'analyser la répartition du temps d'exécution lors du clustering sur les données. 99% du temps de calcul réside dans cette fonction de distance. Ce qui est le plus coûteux en temps est l'accès aux attributs des films. On les a donc réduit au strict minimum. De plus, on ne créé pas de vecteur à 6 coefficients qui stockerait la similitude entre les 2 films pour chaque critère. A la place, on additionne directement le carré de ces valeurs dans une variable de somme, puis on renvoit la racine carrée de cette variable. On utilise la norme 2 en la calculant à la main pour accélérer les calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_distance(movie1, movie2, w_gen=4, w_key=2, w_rel=2, w_pro=3, w_ori=2, w_run=1):\n",
    "    \n",
    "    '''This function computes the distance between 2 movies m1 and m2 given some weight parameters.\n",
    "    It can be called either with 2 lists of attributes or with 2 movie Series, but the 2 parameters\n",
    "    must have the same type.'''\n",
    "    \n",
    "    assert type(movie1) is type(movie2)\n",
    "    sum_vect = 0 # avoiding to store a vector just to compute his norm afterwards\n",
    "    \n",
    "    if isinstance(movie1, pd.Series):\n",
    "        g1, g2 = movie1.genres, movie2.genres\n",
    "        kw1, kw2 = movie1.keywords, movie2.keywords\n",
    "        r1, r2 = movie1.release_date, movie2.release_date\n",
    "        pc1, pc2 = movie1.production_countries, movie2.production_countries\n",
    "        lang1, lang2 = movie1.original_language, movie2.original_language\n",
    "        run1, run2 = movie1.runtime, movie2.runtime\n",
    "    else:\n",
    "        # Access to both movie's attributes stored in the lists movie1 and movie2\n",
    "        g1, g2 = movie1[0], movie2[0]\n",
    "        kw1, kw2 = movie1[1], movie2[1]\n",
    "        r1, r2 = movie1[2], movie2[2]\n",
    "        pc1, pc2 = movie1[3], movie2[3]\n",
    "        lang1, lang2 = movie1[4], movie2[4]\n",
    "        run1, run2 = movie1[5], movie2[5]\n",
    "    \n",
    "    # SIMILARITIES IN GENRES\n",
    "    gen = np.append(g1, g2)\n",
    "    sum_vect += w_gen * (1 - (len(gen) - len(np.unique(gen))) / min(len(g1), len(g2))) ** 2\n",
    "    \n",
    "    # SIMILARITIES IN KEYWORDS\n",
    "    kw = np.append(kw1, kw2)\n",
    "    # Having one keyword in common is sufficient to make 2 films similar for this criterion\n",
    "    # This choice was made because most films have many keywords\n",
    "    if len(kw) == len(np.unique(kw)):\n",
    "        sum_vect += w_key * 1 # ** 2\n",
    "    \n",
    "    # SIMILARITIES FOR THE RELEASE DATE\n",
    "    # the normalized difference between the 2 releade dates\n",
    "    sum_vect += w_rel * (abs(r1 - r2) / MAX_YEAR_DIFFERENCE) ** 2\n",
    "    \n",
    "    # SIMILARITIES IN PRODUCTION COUNTRIES\n",
    "    pc = []\n",
    "    pc.extend(pc1)\n",
    "    pc.extend(pc2)\n",
    "    pc_dist = 1 - (len(pc) - len(np.unique(pc))) / min(len(pc1), len(pc2))\n",
    "    # As it is rare, we set that 2 films which are not from the US have something in common\n",
    "    if 'US' not in pc1 and 'US' not in pc2 and pc_dist > 0.5:\n",
    "        sum_vect += w_pro * 0.5 ** 2\n",
    "    else:\n",
    "        sum_vect += w_pro * pc_dist ** 2\n",
    "    \n",
    "    # SIMILARITIES FOR THE LANGUAGE\n",
    "    if lang1 != lang2 :\n",
    "        # As well, 2 films whose original language is not english have something in common\n",
    "        if lang1 != 'en' and lang2 != 'en':\n",
    "            sum_vect += w_ori * 0.5 ** 2\n",
    "        else:\n",
    "            sum_vect += w_ori * 1 ** 2\n",
    "    \n",
    "    # SIMILARITIES FOR THE RUNTIME\n",
    "    # the normalized difference between the 2 runtimes\n",
    "    sum_vect += w_run * (abs(run1 - run2) / MAX_RUNTIME_DIFFERENCE) ** 2\n",
    "    \n",
    "    return np.sqrt(sum_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on dispose d'une distance entre les films, on doit calculer la matrice de distance entre les films. Pour cela, on va utiliser un DataFrame avec en index et columns les id des movies (en string pour éviter toute confusion avec loc et iloc). On initilise un DataFrame vide avec les bonnes dimensions et les bons index / columns. On le remplit ensuite en faisant appel à la fonction movie_distance pour chaque paire de films. Comme par la suite on veut chercher le coefficient minimum de cette matrice, on met la distance d'un film à lui même à 1000 - un nombre suffisemment grand pour qu'aucune autre valeur de distance ne l'approche avec les choix de poids qu'on a fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_matrix(clu_fea):\n",
    "    \n",
    "    '''This function computes the distance matrix between all the movies contained in clu_fea.\n",
    "    The clu_fea DataFrame must have been cleaned before. Returns the distance matrix.'''\n",
    "    \n",
    "    movies_id = clu_fea.index\n",
    "    dist_mat = pd.DataFrame(np.nan * len(clu_fea), index=movies_id, columns=movies_id)\n",
    "    \n",
    "    # Faire des .loc / .iloc sur un DataFrame prend enormement de temps. Pour eviter cela,\n",
    "    # on le fait pour chaque attribut de chaque film une bonne fois pour toutes, et on stocke\n",
    "    # cela dans une matrice. On est forcé d'utiliser une double liste python car chaque element\n",
    "    # de la matrice peut etre de type int, float, str ou encore list. C'est envrion 4 fois plus\n",
    "    # rapide que de rester avec un DataFrame en accédant aux attributs des films !\n",
    "    \n",
    "    mat_mem = []\n",
    "    for i in range(len(clu_fea)):\n",
    "        mov_i = clu_fea.iloc[i]\n",
    "        mat_mem.append([mov_i.genres, mov_i.keywords, mov_i.release_date,\n",
    "                     mov_i.production_countries, mov_i.original_language, mov_i.runtime])\n",
    "    \n",
    "    # On peut maintenant calculer la distance entre chaque paire de films\n",
    "    # La distance d'un film avec lui meme est fixee a 1000 par defaut\n",
    "    # pour eviter de fusionner un cluster avec lui meme\n",
    "    \n",
    "    for i in range(len(clu_fea)):\n",
    "        for j in range(i, len(clu_fea)):\n",
    "            if i == j:\n",
    "                dist_mat.iat[i, j] = 1000\n",
    "            else:\n",
    "                dist_mat.iat[i, j] = dist_mat.iat[j, i] = movie_distance(mat_mem[i], mat_mem[j])\n",
    "    \n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HIERARCHICAL AGGLOMERATIVE CLUSTERING </b> <br>\n",
    "Dans ce type de méthode de clustering, on n'a pas besoin de préciser le nombre de clusters attendus. L'algorithme permet de construire un dendrogramme, et on obtient nos clusters en coupant le dendrogramme à une certaine hauteur. On va commencer par écrire une classe Dendrogram. Un objet de cette classe contient plusieurs attributs :\n",
    "<li>Un champ leaf - valant None pour les noeuds dans l'arbre et contenant l'id d'un film (int) pour les feuilles </li>\n",
    "<li>Un champ leaf_nb - un int indiquant pour chaque noeud le nombre de feuilles (et donc de films) de l'abre issu de ce noeud </li>\n",
    "<li>Un champ father - une référence vers le père du noeud </li>\n",
    "<li>Un champ left et un champ right - une référence vers le fils gauche (resp. fils droit) du noeud </li>\n",
    "<li>Un champ height - un float indiquant la hauteur du noeud. Attention ! Il ne s'agit pas de la notion classique de hauteur d'un noeud dans un arbre binaire. <br>Il s'agit ici de la hauteur de fusion entre les 2 groupes de films (fils gauche et fils droit). Plus elle est élevée et plus ces 2 groupes de films sont différents </li>\n",
    "<li>Un champ distance_to_father - un float indiquant la longueur de l'arête reliant le noeud à son père </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dendrogram:\n",
    "    \n",
    "    def __init__(self, leaf=None):\n",
    "        \n",
    "        '''This is the Dendrogram class constructor. It takes only one optional argument, which is leaf if the user\n",
    "        wants to build a leaf containing a movie id. Otherwise it is a node and the leaf attribute is set to None.\n",
    "        The other attributes are set to 0 or None for now, and should be modified by setters later on.'''\n",
    "        \n",
    "        self.leaf = leaf\n",
    "        self.leaf_nb = 1\n",
    "        self.father = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.height = 0\n",
    "        self.distance_to_father = 0\n",
    "    \n",
    "    def set_leaf_nb(self):\n",
    "        \n",
    "        '''This method is a setter for the leaf_nb attribute. It requires the left and right sons, as the total number\n",
    "        of leaves of the node is the sum of its left and right leaf_nb attributes. If called on a leaf, leaf_nb should\n",
    "        equal 1 due to the constructor, and set_leaf_nb set leaf_nb to 1 as well. This methode should always be called\n",
    "        after creating a node that is not a leaf.'''\n",
    "        \n",
    "        total_leaf_nb = 0\n",
    "        if self.left is not None:\n",
    "            total_leaf_nb += self.left.leaf_nb\n",
    "        if self.right is not None:\n",
    "            total_leaf_nb += self.right.leaf_nb\n",
    "        self.leaf_nb = max(1, total_leaf_nb)\n",
    "    \n",
    "    def set_height(self, height):\n",
    "        \n",
    "        '''This method is a setter for the height attribute. The height is given as a parameter. This method also sets\n",
    "        the distance_to_father attribute of the node's left and right sons. If called on a leaf, this method does nothing.\n",
    "        This methode should always be called after creating a node that is not a leaf.'''\n",
    "        \n",
    "        if self.left is None or self.right is None:\n",
    "            return\n",
    "        self.height = height\n",
    "        self.left.distance_to_father = height - self.left.height\n",
    "        self.right.distance_to_father = height - self.right.height\n",
    "    \n",
    "    def get_id_list(self):\n",
    "        \n",
    "        '''This method returns a list of all the id contained in the leafs of the node.It uses a prefix run.'''\n",
    "        \n",
    "        id_list = []\n",
    "        def prefix(node):\n",
    "            if node.leaf is not None:\n",
    "                id_list.append(node.leaf)\n",
    "            else:\n",
    "                prefix(node.right)\n",
    "                prefix(node.left)\n",
    "        prefix(self)\n",
    "        \n",
    "        return id_list\n",
    "    \n",
    "    def get_root(self):\n",
    "        \n",
    "        '''This method returns the root of the tree to which the node belongs. The main goal of this method is\n",
    "        to start from a leaf and find the root of the dendrogram, which is the leaf's cluster at a step k.'''\n",
    "        \n",
    "        tmp = self\n",
    "        while tmp.father is not None: tmp = tmp.father\n",
    "        return tmp\n",
    "    \n",
    "    def cut_at_threshold(self, threshold):\n",
    "        \n",
    "        '''This method provides a cut of the dendrogram at a height given by parameter threshold. It returns a list\n",
    "        of Dendrogram objects. Each element of the list can be seen as the root of a dendrogram, i.e. a cluster.'''\n",
    "        \n",
    "        assert threshold >= 0\n",
    "        assert threshold <= self.height\n",
    "        node_list = []\n",
    "        def step(node, t):\n",
    "            if node.height == 0:\n",
    "                node_list.append([node])\n",
    "            else:\n",
    "                if node.left.height <= t:\n",
    "                    node_list.append(node.left)\n",
    "                else:\n",
    "                    step(node.left, t)\n",
    "                if node.right.height <= t:\n",
    "                    node_list.append(node.right)\n",
    "                else:\n",
    "                    step(node.right, t)\n",
    "        step(self, threshold)\n",
    "        \n",
    "        return node_list\n",
    "    \n",
    "    def clusters_threshold_cut(self, threshold):\n",
    "        \n",
    "        '''This method uses cut_at_threshold in order to return a list of movies id list. Each element\n",
    "        of the list is a cluster that directly contains the id of all movies that belong to the cluster.'''\n",
    "        \n",
    "        cluster_list = self.cut_at_threshold(threshold)\n",
    "        return [node.get_id_list() for node in cluster_list]\n",
    "    \n",
    "    def find_best_cut(self):\n",
    "        \n",
    "        '''This method does dendrogram cuts at 200 different threshold and keeps the best cut. It returns a list of\n",
    "        movies id list, i.e. the different clusters. Some changes had to be made to avoid getting too many clusters.\n",
    "        This is due to the distance choice between movies. Some attributes such as director or keywords are almost\n",
    "        all different for 2 movies, resulting in a high distance between most movies, even between 2 movies that are\n",
    "        very similar regarding genres, language, release date, production countries and runtime. That is why the method\n",
    "        imposes a maximum number of clusters, which depends on the size of the input. The upper bound is quite high so\n",
    "        that it enables many clusters, and it avoids to have some situations with e.g. 100 movies and 43 clusters.'''\n",
    "        \n",
    "        max_clu_nb = self.leaf_nb / np.sqrt(self.leaf_nb)\n",
    "        threshold_list = np.linspace(0.01, self.height, 200)\n",
    "        def step(t):\n",
    "            nodes = self.cut_at_threshold(t)\n",
    "            if len(nodes) > max_clu_nb:\n",
    "                return 0\n",
    "            return sum([n.distance_to_father for n in nodes]) / len(nodes)\n",
    "        score = [step(t) for t in threshold_list]\n",
    "        best_threshold_index = np.argmax(score)\n",
    "        best_threshold = threshold_list[best_threshold_index]\n",
    "        \n",
    "        return self.clusters_threshold_cut(best_threshold)\n",
    "    \n",
    "    def get_n_clusters(self, n):\n",
    "        \n",
    "        '''This method provides a cut on the dendrogram that gives a number n of clusters, chosen in parameter.\n",
    "        The different clusters are obtained by cutting at a threshold that leads to n clusters.\n",
    "        It uses dichotomy in order to be efficient. It tries a cut at mid-height, check the number of\n",
    "        clusters obtained and then decides to stop, cut at a higher height or cut at a lower height.'''\n",
    "        \n",
    "        assert n >= 1\n",
    "        assert n <= self.leaf_nb\n",
    "        # dichotomy\n",
    "        xmin, xmax, xmid = 0, self.height, self.height / 2\n",
    "        nodes = self.cut_at_threshold(xmid)\n",
    "        while len(nodes) != n:\n",
    "            if len(nodes) < n: \n",
    "                xmax, xmid = xmid, (xmid + xmin) / 2\n",
    "            else:\n",
    "                xmin, xmid = xmid, (xmid + xmax) / 2\n",
    "            nodes = self.cut_at_threshold(xmid)\n",
    "            # if the loop does not end - extremely unlickely but not impossible if 2 nodes\n",
    "            # have the same height - it calls another cut function that returns n clusters\n",
    "            if xmax - xmin < 1e-5:\n",
    "                return self.get_n_clusters_perso(n)[0]\n",
    "        \n",
    "        return [n.get_id_list() for n in nodes]\n",
    "        \n",
    "    def get_n_clusters_perso(self, n):\n",
    "        \n",
    "        '''This method provides a cut on the dendrogram that gives a number n of clusters, chosen in parameter.\n",
    "        It is called get_n_clusters_perso because I imaginated it alone and I don't think it exists elsewhere.\n",
    "        The goal of this method is to solve the following problem : most of the time, cutting a dendrogram at\n",
    "        a given height leads to a certain number of clusters. Among these clusters, some can be very small, there\n",
    "        are even clusters with one element. So, instead of cutting at a threshold, the idea of this method is the\n",
    "        following : it starts at the root and must provide n clusters, e.g. let's take n=10. If the left son's leaf\n",
    "        number is greater than the right son's one, then the left son must provide let's say n=7 clusters, while the\n",
    "        right son must provide n=3 clusters. Finally if the right son's have a very small number of leaves compared\n",
    "        to the left son's one, then the left son must provide n=10 clusters and the leaves of the right son are added\n",
    "        to a list of outliers. At the end of the process, there will be 10 clusters quite well balanced and one list\n",
    "        of outliers, which can form a special cluster of, let's say, unclassifiable movies.'''\n",
    "        \n",
    "        if n <= 0 or n > self.leaf_nb:\n",
    "            print(\"Bad choice for n : too big or <= 0\")\n",
    "            return\n",
    "        cluster_list = []\n",
    "        outliers = []\n",
    "        error = []\n",
    "        def step(node, n):\n",
    "            if n == 1:\n",
    "                cluster_list.append(node.get_id_list())\n",
    "            elif node.left is None or node.right is None:\n",
    "                error.append(True)\n",
    "            else:\n",
    "                prop_left = node.left.leaf_nb / node.leaf_nb\n",
    "                prop_right = node.right.leaf_nb / node.leaf_nb\n",
    "                # a node is considered an outlier if his leaf number\n",
    "                # is less than 15% of his father's leaf number\n",
    "                if prop_left < 0.15:\n",
    "                    outliers.extend(node.left.get_id_list())\n",
    "                    step(node.right, n)\n",
    "                elif prop_right < 0.15:\n",
    "                    outliers.extend(node.right.get_id_list())\n",
    "                    step(node.left, n)\n",
    "                else:\n",
    "                    n_left = max(1, round(n * prop_left))\n",
    "                    if n_left == n:\n",
    "                        n_left -= 1\n",
    "                    n_right = n - n_left\n",
    "                    step(node.left, n_left)\n",
    "                    step(node.right, n_right)\n",
    "        step(self, n)\n",
    "        if error:\n",
    "            print(\"n too big\")\n",
    "        else:\n",
    "            return cluster_list, outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut désormais écrire une classe pour implémenter le Hierarchical Agglomerative Clustering. Un objet de cette classe contiendra les attributs suivants :<br>\n",
    "<li>Un champ dendrogram_root - une référence vers la racine du dendrogramme</li>\n",
    "<li>Un champ cluster_id_list - une liste de listes de movies id, i.e. la liste des clusters</li>\n",
    "<li>Un champ outliers - une liste de movies id, existant uniquement si on choisi la méthode perso</li>\n",
    "<li>Un champ cluster_features - le DataFrame des films nettoyé sur lequel on apprend </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClusterRecommender:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        '''This is the HierarchicalCluster class constructor. It takes no arguments, all it does is to\n",
    "        build an object. The 4 attributes will be initialized later on, when fiting the model.'''\n",
    "        \n",
    "        self.dendrogram_root = None\n",
    "        self.cluster_id_list = None\n",
    "        self.outliers = None\n",
    "        self.cluster_features = None\n",
    "    \n",
    "    def set_cluster_features(self, dfm):\n",
    "        \n",
    "        '''This method is a setter for the attribute cluster_features. It takes one parameter, dfm, which is\n",
    "        a DataFrame that contains all the information about movies. The method cleans the DataFrame and keeps\n",
    "        only the attributes that are relevant for the movie clustering.'''\n",
    "        \n",
    "        # selection des attributs qui nous interessent pour le clustering\n",
    "        clu_fea = dfm[['genres','keywords','release_date','production_countries','original_language','runtime']]\n",
    "        # on met en index les id des movies\n",
    "        clu_fea.index = dfm.movieId.apply(lambda x: str(x))\n",
    "        # nettoyage du dataframe\n",
    "        clu_fea = clean_runtime(clu_fea)\n",
    "        clu_fea = drop_missing_values(clu_fea)\n",
    "        vectorize_keywords(clu_fea)\n",
    "        vectorize_genres(clu_fea)\n",
    "        simplify_date(clu_fea)\n",
    "        simplify_countries(clu_fea)\n",
    "        self.cluster_features = clu_fea\n",
    "    \n",
    "    def agglomerative_cluster(self, dist_mat):\n",
    "        \n",
    "        '''This method builds a dendrogram based on the distance matrix given in parameters. It returns\n",
    "        its root. Important things to know about this function : 1) In order to avoid redundancy and to make\n",
    "        the computations faster, it reduces the matrix size at each step by dropping a row and a colums.\n",
    "        At the end, the matrix has 1x1 shape, so if one wants to store the matrix and keep it unchanged,\n",
    "        he must call this method with a copy. 2) A choice which has a huge impact on the dendrogram was made\n",
    "        here. In the algorithm, the 2 closest clusters are merged into a bigger one at each step. But there are\n",
    "        several ways to measure the distance between clusters. It can be the distance between the centroids, the\n",
    "        average distance, the minimal or maximal distance bewteen 2 points from different clusters. We chose this\n",
    "        last option. It avoids to have many unbalanced branches, especially at levels close to the root.'''\n",
    "        \n",
    "        assert self.cluster_features is not None\n",
    "        clu_fea = self.cluster_features\n",
    "        clu_fea[\"dendrogram\"] = clu_fea.index\n",
    "        clu_fea.dendrogram = clu_fea.dendrogram.apply(lambda x: Dendrogram(leaf=int(x)))\n",
    "        size_mat = len(clu_fea)\n",
    "        for _ in range(1, size_mat):\n",
    "            # localisation de la plus petite distance dans la matrice\n",
    "            index_str1, index_str2 = dist_mat.stack().idxmin()\n",
    "            height = dist_mat.loc[index_str1, index_str2]\n",
    "            mov1 = clu_fea.loc[index_str1]\n",
    "            mov2 = clu_fea.loc[index_str2]\n",
    "            tmp1 = mov1.dendrogram\n",
    "            tmp2 = mov2.dendrogram\n",
    "            # acces a la racine du cluster de mov1 et de celui de mov2\n",
    "            while tmp1.father is not None: tmp1 = tmp1.father\n",
    "            while tmp2.father is not None: tmp2 = tmp2.father\n",
    "            # creation de la racine du nouvel arbre, fusion des 2 clusters\n",
    "            tmp3 = Dendrogram()\n",
    "            tmp3.left = tmp1\n",
    "            tmp3.right = tmp2\n",
    "            tmp1.father = tmp3\n",
    "            tmp2.father = tmp3\n",
    "            tmp3.set_leaf_nb()\n",
    "            tmp3.set_height(height)\n",
    "            # actualisation de la matrice de distance\n",
    "            new_d = np.maximum(dist_mat.loc[index_str1, :], dist_mat.loc[index_str2, :])\n",
    "            dist_mat.loc[index_str1, :] = dist_mat.loc[:, index_str1] = new_d\n",
    "            # suppression d'une des 2 lignes et colonnes qui font maintenant doublons\n",
    "            dist_mat = dist_mat.drop(index_str2, axis=0)\n",
    "            dist_mat = dist_mat.drop(index_str2, axis=1)\n",
    "        \n",
    "        return clu_fea.iloc[0].dendrogram.get_root()\n",
    "\n",
    "    \n",
    "    def set_n_clusters(self, n):\n",
    "        \n",
    "        '''This method cuts the dendrogram at a height that forms n different clusters.'''\n",
    "    \n",
    "        assert self.dendrogram_root is not None, \"Erreur, vous devez d'abord construire le dendrogram avec la methode fit.\"\n",
    "        self.outliers = None\n",
    "        self.cluster_id_list = self.dendrogram_root.get_n_clusters(n)\n",
    "    \n",
    "    def set_best_n_clusters(self):\n",
    "        \n",
    "        '''This method cuts the dendrogram at a height that maximizes the distance_to_father attributes\n",
    "        of the cluster roots, on the condition that it doesn't create too many clusters.'''\n",
    "        \n",
    "        assert self.dendrogram_root is not None, \"Erreur, vous devez d'abord construire le dendrogram avec la methode fit.\"\n",
    "        self.outliers = None\n",
    "        self.cluster_id_list = self.dendrogram_root.find_best_cut()\n",
    "    \n",
    "    def set_n_clusters_perso(self, n):\n",
    "        \n",
    "        '''This method cuts the dendrogram at different heights depending on the branch, so that if forms\n",
    "        n clusters quite well balanced. It does not take the nodes height in consideration.'''\n",
    "        \n",
    "        assert self.dendrogram_root is not None, \"Erreur, vous devez d'abord construire le dendrogram avec la methode fit.\"\n",
    "        self.cluster_id_list, self.outliers = self.dendrogram_root.get_n_clusters_perso(n)\n",
    "            \n",
    "    def fit(self, dfm_cluster, method='set_best_n_clusters', n=None):\n",
    "        \n",
    "        '''This method coordinates the cluster construction. Firstly it cleans the DataFrame by calling the\n",
    "        set_cluster_features method. Then it calls the function that computes the distance matrix. Finally\n",
    "        it calls the agglomerative_cluster method in order to build the dendrogram. If fit is called twice\n",
    "        or more times, for example in order to change the cut method or parameters, the dendrogram remains\n",
    "        unchanged to avoid doing all computations again and for nothing. If one wants to change the dataset,\n",
    "        the attribute dendrogram root has to be set to None.'''\n",
    "        \n",
    "        assert (method == 'set_best_n_clusters' and n is None) or (method != 'set_best_n_clusters' and n is not None)\n",
    "        \n",
    "        if self.dendrogram_root is None:\n",
    "            self.set_cluster_features(dfm_cluster)\n",
    "            dist_mat = compute_dist_matrix(self.cluster_features)\n",
    "            self.dendrogram_root = self.agglomerative_cluster(dist_mat)\n",
    "        \n",
    "        if (method == 'set_best_n_clusters'):\n",
    "            self.set_best_n_clusters()\n",
    "        elif (method == 'set_n_clusters'):\n",
    "            self.set_n_clusters(n)\n",
    "        elif (method == 'set_n_clusters_perso'): \n",
    "            self.set_n_clusters_perso(n)\n",
    "        else:\n",
    "            print(\"Erreur, Fonction pas reconnue\")\n",
    "    \n",
    "    def get_cluster(self, pos, movies=None):\n",
    "        \n",
    "        '''This method returns a DataFrame that contains the movies from cluster number pos (first pos is 1).\n",
    "        It uses the self.cluster_features attribute if the argument movies is not precised.'''\n",
    "        \n",
    "        if movies is None:\n",
    "            movies = self.cluster_features\n",
    "        if pos <= 0:\n",
    "            print(\"Erreur, l'argument doit etre >= 1\")\n",
    "            return\n",
    "        if pos > len(self.cluster_id_list):\n",
    "            print(\"Erreur, il y a seulement \", len(self.cluster_id_list), \" clusters.\")\n",
    "            return\n",
    "        if self.cluster_id_list is None:\n",
    "            print(\"Erreur, vous devez d'abord choisir un nombre de clusters avec la methode set_n_clusters.\")\n",
    "            return\n",
    "        df = pd.DataFrame([])\n",
    "        if 'movieId' in movies.columns:\n",
    "            for i in self.cluster_id_list[pos - 1]:\n",
    "                df = df.append(movies[movies.movieId == int(i)])\n",
    "        else:\n",
    "            for i in self.cluster_id_list[pos - 1]:\n",
    "                df = df.append(movies.loc[str(i)])\n",
    "        return df\n",
    "    \n",
    "    def get_outliers(self, movies=None):\n",
    "        \n",
    "        '''This method returns a DataFrame that contains all the outliers movies. It exists only if the perso\n",
    "        method is used when setting a number of clusters. Otherwise it returns an empty DataFrame.'''\n",
    "        \n",
    "        df = pd.DataFrame([])\n",
    "        if self.outliers is None:\n",
    "            return df\n",
    "        if movies is None:\n",
    "            movies = self.cluster_features\n",
    "        if 'movieId' in movies.columns:\n",
    "            for i in outliers:\n",
    "                df = df.append(movies[movies.movieId == int(i)])\n",
    "        else:\n",
    "            for i in outliers:\n",
    "                df = df.append(movies.loc[str(i)])\n",
    "        return df\n",
    "    \n",
    "    def get_clusters_size(self):\n",
    "        \n",
    "        '''This method returns a list containing the size of the clusters,\n",
    "        i.e. the number of movies for each cluster.'''\n",
    "        \n",
    "        if self.cluster_id_list is None:\n",
    "            print(\"Erreur, vous devez d'abord choisir un nombre de clusters avec la methode set_n_clusters.\")\n",
    "            return\n",
    "        l = []\n",
    "        for id_list in self.cluster_id_list:\n",
    "            l.append(len(id_list))\n",
    "        return l\n",
    "    \n",
    "    def find_closest_cluster(self, movie):\n",
    "        \n",
    "        '''This method finds the closest cluster for a movie given in parameter. The closest cluster is the\n",
    "        cluster that contains the movie that is the closest to the argument.'''\n",
    "        \n",
    "        if self.cluster_id_list is None:\n",
    "            print(\"Erreur, vous devez d'abord choisir un nombre de clusters avec la methode set_n_clusters.\")\n",
    "            return\n",
    "        min_list = []\n",
    "        for cluster in self.cluster_id_list:\n",
    "            min_dist = np.min([movie_distance(movie, self.cluster_features.loc[str(m)]) for m in cluster])\n",
    "            min_list.append(min_dist)\n",
    "        index_closest_cluser = np.argmin(min_list)\n",
    "        \n",
    "        return self.cluster_id_list[index_closest_cluser]\n",
    "    \n",
    "    def _best_(self, uid, ratings):\n",
    "        '''Renvoie un array des id des films pour lesquels l'utilisateur uid a mis une note de plus de 3/5'''\n",
    "        watched_movies = ratings[ratings.userId == uid]\n",
    "        top_movies = watched_movies[watched_movies.rating >= 3]\n",
    "       \n",
    "        if len(top_movies)==0:\n",
    "            print(\"Erreur, cet utilisateur n'a pas donné de note au desss de 3/5. :(\")\n",
    "            return\n",
    "       \n",
    "        top_movies = top_movies[[\"movieId\"]].to_numpy()\n",
    "        return top_movies\n",
    "         \n",
    "    def recommend(self, uid, ratings, nb_reco=20):\n",
    "        '''Retourne le cluster qui contient le plus des films bien notés de l'utilisateur uid'''\n",
    "        assert self.cluster_id_list is not None\n",
    "        \n",
    "        top_movies = self._best_(uid, ratings)\n",
    "        \n",
    "        nb_cluster = len(self.cluster_id_list)\n",
    "       \n",
    "        L = np.zeros(nb_cluster)\n",
    "        \n",
    "        for movie in top_movies:\n",
    "            for i in range(nb_cluster):\n",
    "                if movie in self.cluster_id_list[i] :\n",
    "                    L[i] += 1\n",
    "                break\n",
    "       \n",
    "        indice = np.argmax(L)\n",
    "        if len(self.cluster_id_list[indice]) > nb_reco:\n",
    "            return self.cluster_id_list[indice][:nb_reco]\n",
    "        return self.cluster_id_list[indice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va illustrer sur un échantillon de taille 10 les 3 méthodes proposées pour partitionner le dendrogramme en clusters. Note : on réalise un sample sur le DataFrame dfm_cluster. Le nettoyage des données s'effectuera dans les méthodes de classe, ainsi certains films du sample pourront avoir été retirés, c'est pourquoi le nombre total de film tout cluster confondu peut être inférieur à 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10 = HierarchicalClusterRecommender()\n",
    "test_10 = dfm_cluster.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91.2 ms, sys: 3.01 ms, total: 94.2 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "%time cluster_10.fit(test_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première méthode coupe le dendrogramme de façon à obtenir un nombre de clusters désiré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.set_n_clusters(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>[35, 18]</td>\n",
       "      <td>[331, 6027, 13027, 33624, 162724]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>The Suburbans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>[18, 10749]</td>\n",
       "      <td>[10241, 15188, 15274, 208634, 208635]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Losing Chase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>[12, 14, 18, 35, 10402]</td>\n",
       "      <td>[637, 1740, 3490]</td>\n",
       "      <td>en</td>\n",
       "      <td>[GB]</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Spice World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27005</th>\n",
       "      <td>[18]</td>\n",
       "      <td>[703, 4434, 6149]</td>\n",
       "      <td>en</td>\n",
       "      <td>[AU]</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>The Interview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        genres                               keywords  \\\n",
       "3001                  [35, 18]      [331, 6027, 13027, 33624, 162724]   \n",
       "1531               [18, 10749]  [10241, 15188, 15274, 208634, 208635]   \n",
       "1760   [12, 14, 18, 35, 10402]                      [637, 1740, 3490]   \n",
       "27005                     [18]                      [703, 4434, 6149]   \n",
       "\n",
       "      original_language production_countries  release_date  runtime  \\\n",
       "3001                 en                 [US]        1999.0     81.0   \n",
       "1531                 en                 [US]        1996.0     98.0   \n",
       "1760                 en                 [GB]        1997.0     93.0   \n",
       "27005                en                 [AU]        1998.0    104.0   \n",
       "\n",
       "               title  \n",
       "3001   The Suburbans  \n",
       "1531    Losing Chase  \n",
       "1760     Spice World  \n",
       "27005  The Interview  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(1, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>[878]</td>\n",
       "      <td>[402, 2964, 9826, 11628, 18069, 162988, 187046...</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>The 6th Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>[27, 53]</td>\n",
       "      <td>[9663, 11545, 34079]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Leatherface: Texas Chainsaw Massacre III</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genres                                           keywords  \\\n",
       "3986     [878]  [402, 2964, 9826, 11628, 18069, 162988, 187046...   \n",
       "2461  [27, 53]                               [9663, 11545, 34079]   \n",
       "\n",
       "     original_language production_countries  release_date  runtime  \\\n",
       "3986                en                 [US]        2000.0    123.0   \n",
       "2461                en                 [US]        1990.0     81.0   \n",
       "\n",
       "                                         title  \n",
       "3986                               The 6th Day  \n",
       "2461  Leatherface: Texas Chainsaw Massacre III  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(2, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31410</th>\n",
       "      <td>[18, 36, 10752]</td>\n",
       "      <td>[220, 351, 407, 1443, 1698, 1956, 2052, 2300, ...</td>\n",
       "      <td>de</td>\n",
       "      <td>[AT, DE, IT]</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>Downfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>[35]</td>\n",
       "      <td>[2587, 3096, 4481, 5809, 15160, 172471]</td>\n",
       "      <td>de</td>\n",
       "      <td>[DE]</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>The Superwife</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                genres                                           keywords  \\\n",
       "31410  [18, 36, 10752]  [220, 351, 407, 1443, 1698, 1956, 2052, 2300, ...   \n",
       "651               [35]            [2587, 3096, 4481, 5809, 15160, 172471]   \n",
       "\n",
       "      original_language production_countries  release_date  runtime  \\\n",
       "31410                de         [AT, DE, IT]        2004.0    156.0   \n",
       "651                  de                 [DE]        1996.0     86.0   \n",
       "\n",
       "               title  \n",
       "31410       Downfall  \n",
       "651    The Superwife  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(3, cluster_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième méthode aboutit à un nombre de clusters désiré, en faisant en sorte que les tailles des clusters ne soient pas disproportionnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.set_n_clusters_perso(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>[35, 18]</td>\n",
       "      <td>[331, 6027, 13027, 33624, 162724]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>The Suburbans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>[18, 10749]</td>\n",
       "      <td>[10241, 15188, 15274, 208634, 208635]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Losing Chase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>[12, 14, 18, 35, 10402]</td>\n",
       "      <td>[637, 1740, 3490]</td>\n",
       "      <td>en</td>\n",
       "      <td>[GB]</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Spice World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27005</th>\n",
       "      <td>[18]</td>\n",
       "      <td>[703, 4434, 6149]</td>\n",
       "      <td>en</td>\n",
       "      <td>[AU]</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>The Interview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        genres                               keywords  \\\n",
       "3001                  [35, 18]      [331, 6027, 13027, 33624, 162724]   \n",
       "1531               [18, 10749]  [10241, 15188, 15274, 208634, 208635]   \n",
       "1760   [12, 14, 18, 35, 10402]                      [637, 1740, 3490]   \n",
       "27005                     [18]                      [703, 4434, 6149]   \n",
       "\n",
       "      original_language production_countries  release_date  runtime  \\\n",
       "3001                 en                 [US]        1999.0     81.0   \n",
       "1531                 en                 [US]        1996.0     98.0   \n",
       "1760                 en                 [GB]        1997.0     93.0   \n",
       "27005                en                 [AU]        1998.0    104.0   \n",
       "\n",
       "               title  \n",
       "3001   The Suburbans  \n",
       "1531    Losing Chase  \n",
       "1760     Spice World  \n",
       "27005  The Interview  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(1, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>[878]</td>\n",
       "      <td>[402, 2964, 9826, 11628, 18069, 162988, 187046...</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>The 6th Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>[27, 53]</td>\n",
       "      <td>[9663, 11545, 34079]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Leatherface: Texas Chainsaw Massacre III</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genres                                           keywords  \\\n",
       "3986     [878]  [402, 2964, 9826, 11628, 18069, 162988, 187046...   \n",
       "2461  [27, 53]                               [9663, 11545, 34079]   \n",
       "\n",
       "     original_language production_countries  release_date  runtime  \\\n",
       "3986                en                 [US]        2000.0    123.0   \n",
       "2461                en                 [US]        1990.0     81.0   \n",
       "\n",
       "                                         title  \n",
       "3986                               The 6th Day  \n",
       "2461  Leatherface: Texas Chainsaw Massacre III  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(2, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31410</th>\n",
       "      <td>[18, 36, 10752]</td>\n",
       "      <td>[220, 351, 407, 1443, 1698, 1956, 2052, 2300, ...</td>\n",
       "      <td>de</td>\n",
       "      <td>[AT, DE, IT]</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>Downfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>[35]</td>\n",
       "      <td>[2587, 3096, 4481, 5809, 15160, 172471]</td>\n",
       "      <td>de</td>\n",
       "      <td>[DE]</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>The Superwife</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                genres                                           keywords  \\\n",
       "31410  [18, 36, 10752]  [220, 351, 407, 1443, 1698, 1956, 2052, 2300, ...   \n",
       "651               [35]            [2587, 3096, 4481, 5809, 15160, 172471]   \n",
       "\n",
       "      original_language production_countries  release_date  runtime  \\\n",
       "31410                de         [AT, DE, IT]        2004.0    156.0   \n",
       "651                  de                 [DE]        1996.0     86.0   \n",
       "\n",
       "               title  \n",
       "31410       Downfall  \n",
       "651    The Superwife  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(3, cluster_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin la troisième méthode vise à trouver la coupe qui maximise la distance entre les clusters. Cependant la coupe qui maximise cette distance n'est pas forcément la plus adaptée en raison du choix de la distance sur les films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.set_best_n_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>[878]</td>\n",
       "      <td>[402, 2964, 9826, 11628, 18069, 162988, 187046...</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>The 6th Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>[27, 53]</td>\n",
       "      <td>[9663, 11545, 34079]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Leatherface: Texas Chainsaw Massacre III</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>[35, 18]</td>\n",
       "      <td>[331, 6027, 13027, 33624, 162724]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>The Suburbans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>[18, 10749]</td>\n",
       "      <td>[10241, 15188, 15274, 208634, 208635]</td>\n",
       "      <td>en</td>\n",
       "      <td>[US]</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>Losing Chase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>[12, 14, 18, 35, 10402]</td>\n",
       "      <td>[637, 1740, 3490]</td>\n",
       "      <td>en</td>\n",
       "      <td>[GB]</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Spice World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27005</th>\n",
       "      <td>[18]</td>\n",
       "      <td>[703, 4434, 6149]</td>\n",
       "      <td>en</td>\n",
       "      <td>[AU]</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>The Interview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        genres  \\\n",
       "3986                     [878]   \n",
       "2461                  [27, 53]   \n",
       "3001                  [35, 18]   \n",
       "1531               [18, 10749]   \n",
       "1760   [12, 14, 18, 35, 10402]   \n",
       "27005                     [18]   \n",
       "\n",
       "                                                keywords original_language  \\\n",
       "3986   [402, 2964, 9826, 11628, 18069, 162988, 187046...                en   \n",
       "2461                                [9663, 11545, 34079]                en   \n",
       "3001                   [331, 6027, 13027, 33624, 162724]                en   \n",
       "1531               [10241, 15188, 15274, 208634, 208635]                en   \n",
       "1760                                   [637, 1740, 3490]                en   \n",
       "27005                                  [703, 4434, 6149]                en   \n",
       "\n",
       "      production_countries  release_date  runtime  \\\n",
       "3986                  [US]        2000.0    123.0   \n",
       "2461                  [US]        1990.0     81.0   \n",
       "3001                  [US]        1999.0     81.0   \n",
       "1531                  [US]        1996.0     98.0   \n",
       "1760                  [GB]        1997.0     93.0   \n",
       "27005                 [AU]        1998.0    104.0   \n",
       "\n",
       "                                          title  \n",
       "3986                                The 6th Day  \n",
       "2461   Leatherface: Texas Chainsaw Massacre III  \n",
       "3001                              The Suburbans  \n",
       "1531                               Losing Chase  \n",
       "1760                                Spice World  \n",
       "27005                             The Interview  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(1, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31410</th>\n",
       "      <td>[18, 36, 10752]</td>\n",
       "      <td>[220, 351, 407, 1443, 1698, 1956, 2052, 2300, ...</td>\n",
       "      <td>de</td>\n",
       "      <td>[AT, DE, IT]</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>Downfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>[35]</td>\n",
       "      <td>[2587, 3096, 4481, 5809, 15160, 172471]</td>\n",
       "      <td>de</td>\n",
       "      <td>[DE]</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>The Superwife</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                genres                                           keywords  \\\n",
       "31410  [18, 36, 10752]  [220, 351, 407, 1443, 1698, 1956, 2052, 2300, ...   \n",
       "651               [35]            [2587, 3096, 4481, 5809, 15160, 172471]   \n",
       "\n",
       "      original_language production_countries  release_date  runtime  \\\n",
       "31410                de         [AT, DE, IT]        2004.0    156.0   \n",
       "651                  de                 [DE]        1996.0     86.0   \n",
       "\n",
       "               title  \n",
       "31410       Downfall  \n",
       "651    The Superwife  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_10.get_cluster(2, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur, il y a seulement  2  clusters.\n"
     ]
    }
   ],
   "source": [
    "cluster_10.get_cluster(3, cluster_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant utiliser un échantillon de taille 100 pour comparer la taille des clusters obtenus par ces 3 méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_100 = dfm_cluster.sample(100)\n",
    "cluster_100 = HierarchicalClusterRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 27.7 ms, total: 1.45 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%time cluster_100.fit(test_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des clusters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[28, 28, 12, 10, 8, 1]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# premiere methode\n",
    "cluster_100.set_n_clusters(6)\n",
    "print(\"Taille des clusters\")\n",
    "cluster_100.get_clusters_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  outliers\n",
      "Taille des clusters\n",
      "[21, 5, 23, 5, 12, 10]\n"
     ]
    }
   ],
   "source": [
    "# deuxieme methode\n",
    "cluster_100.set_n_clusters_perso(6)\n",
    "print(len(cluster_100.outliers), \" outliers\")\n",
    "print(\"Taille des clusters\")\n",
    "print(cluster_100.get_clusters_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des clusters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[28, 23, 3, 2, 6, 6, 10, 8, 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# troisieme methode\n",
    "cluster_100.set_best_n_clusters()\n",
    "print(\"Taille des clusters\")\n",
    "cluster_100.get_clusters_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième méthode permet en effet d'avoir des clusters de taille équilibrée et on peut songer à l'utiliser si les autres méthodes de coupe ne donnent pas une partition satisfaisante pour la recommendation hybride. Sinon on peut utiliser la troisième méthode, regarder la taille et le nombre de clusters et décider si on veut appeler la première méthode pour augmenter ou réduire le nombre de clusters.<br> <br>\n",
    "<b>On va maintenant effectuer le clustering sur toutes les données</b>. Après nettoyage, il restera un peu plus de 8000 films. Cela va prendre environ 2 heures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-based recommendation : user- et item- based <a id='memory'></a>\n",
    "\n",
    "[retour au sommaire](#sommaire)\n",
    "\n",
    "Pour prédire la note d'un couple (*user*, *movie*) on peut regarder quelle note les utilisateurs similaires à *user* ont donné à ce film et faire une moyenne de leurs notes. On peut également regarder quelle note *user* a donné à des films similaires à *movie*. La première approche est centré sur les utilisateurs, *user-based*, tandis que la deuxième est centrée sur les films, *item-based*. Néanmoins les deux approches suivent la même logique et nous allons implémenter des fonctions qui s'adaptent en fonction de l'approche choisie. Dans un système *user-based*, nous allons appeler **peers** les **users** et **others** les **items**. Dans un système *item-based* c'est l'inverse.\n",
    "\n",
    "Deux utilisateurs sont considérés comme similaires s'ils ont les mêmes préférences de films. Il semble en effet plus pertinent de demander à un utilisateurs aux goûts similaires à *user* de lui conseiller un film. Pour comparer deux utilisateurs il faudra donc regarder les notes qu'ils ont donné aux mêmes films. De manière analogue, deux films sont similaires s'ils sont appréciés par les mêmes utilisateurs. Il faudra donc regarder les notes données par les mêmes utilisateurs pour comparer deux films. Cette notion de similitude sera calculée par un taux de corrélation.\n",
    "\n",
    "Nous rappelons ici l'algorithme proposé dans notre cours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation des notes\n",
    "\n",
    "Nous n'avons besoin pour ce système que des notes données par les utilisateurs. Nous utiliserons donc seulement la table `ratings`. Puisque la moyenne des notes varie d'un utilisateur à un autre et d'un film à un autre, nous devons translater les notes afin que la moyenne des notes se trouve à 0. En *user-based*, on considère la moyenne par utilisateur, tandis qu'en *item-based* on s'interèsse à la moyenne par film. Par abus de langage nous appelons ces nouvelles notes les notes *normalisées*. Cette normalisation se fait via la méthode `_normalize_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taux de corrélation\n",
    "\n",
    "\n",
    "Dans un système *user-based*, on note $I_u$ l'ensemble des items renseignés pour l'utilisateur $u$ et $U_k$ l'ensemble des utilisateurs qui ont notés le film $k$. On note $I_{uv} = I_u \\cap I_v$. Pour le *item-based* on utilisera les mêmes notations en intervertissant user et item. On notera également $s_{ui}$ la note normalisée de l'item *i* donnée par l'utilisateur *u*. \n",
    "\n",
    "\n",
    "Pour déterminer si deux utilisateurs se ressemblent en termes de goûts, nous utilisons un taux de corrélation sur les avis données. L'utilisateur peut indiquer s'il veut ajuster le taux et/ou le discount. Ainsi le taux de correlation classique est donné par la formule :\n",
    "$$\n",
    "cor(u, v) = \\frac{\\sum_{k \\in I_{uv}} s_{uk} s_{vk}}{\\sqrt{\\sum_{k \\in I_{uv}} s_{uk}^2}\\sqrt{\\sum_{k \\in I_{uv}} s_{vk}^2}}\n",
    "$$\n",
    "\n",
    "Si on veut ajuster la corrélation pour ne pas donner trop d'importance aux films populaires vu par la majorité des personnes, on utilise la formule suivante :\n",
    "\n",
    "$$\n",
    "cor\\_adj(u, v) = \\frac{\\sum_{k \\in I_{uv}} s_{uk} s_{vk} /| U_k| }{\\sqrt{\\sum_{k \\in I_{uv}} \\frac{s_{uk}^2}{|U_k|}}\\sqrt{\\sum_{k \\in I_{uv}} \\frac{s_{vk}^2}{|U_k|}}}\n",
    "$$\n",
    "\n",
    "Enfin, si on ne veut pas donner une correlation trop élevée si les deux utilisateurs n'ont pas donné assez d'avis sur des films en commun, on utilisera la formule suivante.\n",
    "$$\n",
    "cor\\_dis(u, v) = correlation * \\frac{min(|I_{uv}|, \\beta)}{\\beta}\n",
    "$$\n",
    "$\\beta$ est le nombre minimal de films que deux utilisateurs doivent avoir notés en commun. \n",
    "La variable $correlation$ peut être calculée de manière classique ou ajustée.\n",
    "\n",
    "L'implémentation que nous utilisons pour la version ajustée de la corrélation ne nous semble pas assez optimale. En faisant des petits tests sur la version classique et la version ajustée, cette dernière peut prendre jusqu'à 2 fois plus de temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de corrélation\n",
    "\n",
    "Nous avons fait le choix d'utiliser un numpy array pour la matrice de corrélation. Puisque celle-ci est symétrique, nous stockons les données en double. Une première démarche avait été d'utiliser une dataframe à double indice où la valeur du premier indice était strictement inférieure à la valeur du deuxième. Néanmoins l'accès à une valeur dans une telle dataframe est beaucoup plus lente que dans un numpy array : la complexité est constante dans la numpy array. Nous avons préféré sacrifier un peu de compléxité en espace pour gagner considérablement en temps d'exécution.\n",
    "\n",
    "En utilsant une matrice, l'accès à un taux de corrélation se fait par des indices. Or il se peut que les id des peers ne soient pas consécutifs. Ceci est par exemple le cas lorsqu'on travaille avec un échantillon des données ou que des lignes ont été supprimées lors d'un nettoyage des données. Il faut alors établir une correspondance entre les id et les indices, que nous nommerons rank pour éviter la confusion orthographique. Pour cela nous utilisons un dictionnaire `id_rank` dont les clefs sont les id et les valeurs les rangs associés. \n",
    "\n",
    "\n",
    "La fonction de corrélation à utiliser doit être précisée. Nous utilisons également le module logging pour permettre le suivi du déroulement du calcul si précisé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction\n",
    "\n",
    "Pour prédire la note qu'un utilisateur donnera à un film, nous faisons un moyenne des notes données pour les k *peer* les plus proches. Dans une approche *user-based*, on regarde donc les k plus proches utilisateurs, dans une approche *item-based*, les k films les plus proches. Les plus proches sont ceux qui ont une corrélation la plus élevée. On appelle **p** le peer et **o** l'élément other.\n",
    "\n",
    "La moyenne effectuée est pondérée par les coefficients de corrélations. On ajoute également la moyenne des notes de **p** pour retrouver une note non normalisée correspondant à l'échelle de note que l'utilisateur a tendance à donner.\n",
    "\n",
    "La prédiction de la note $\\hat{\\sigma}_{um}$ de l'user $u$ au film $m$ est :\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{um} = \\mu_u + \\frac{\\sum_{v \\in P_u(m)} s_{vm} \\cdot cor(u, v)}{\\sum_{v \\in P_u(m)} |cor(u,v)|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MemoryBasedPredictor:\n",
    "    def __init__(self):\n",
    "        self.base = None\n",
    "        self.ptype = None\n",
    "        self.otype = None\n",
    "        self.dfr = None\n",
    "        self.cm = None\n",
    "        self.id_rank = None # explain what this is for\n",
    "        self.mean_peers = None\n",
    "    \n",
    "    def fit(self, ratings, base='user', adjust=False, discount=0, verbose=False):\n",
    "        '''\n",
    "        Construit la matrice de correlation entre tous les peers présents dans ratings\n",
    "        \n",
    "        :param: ratings la dataframe contenant les notes des utilisateurs donnés aux films\n",
    "                adjust - un booléen indiquant s'il faut ajuster la corrélation\n",
    "                discount - un entier indiquant le seuil de other en commun à avoir (paramètre beta)\n",
    "                        donner 0 pour indiquer de ne pas utiliser la variante discounted\n",
    "                verbose un booléen indiquant s'il faut afficher le suivi des calculs\n",
    "        '''\n",
    "        assert all(x in ratings.columns for x in ['userId', 'movieId', 'rating'])\n",
    "        assert base in {'user', 'movie'}\n",
    "        \n",
    "        if self.cm is not None: #already fitted\n",
    "            return\n",
    "        \n",
    "        self.base = base\n",
    "        self.ptype, self.otype = ('userId', 'movieId') if self.base == 'user' else ('movieId', 'userId')\n",
    "        \n",
    "        self.dfr = ratings[['userId', 'movieId', 'rating']].set_index([self.ptype, self.otype])\n",
    "        self.dfr.sort_index(inplace=True)\n",
    "\n",
    "        # normalize ratings per peer\n",
    "        self.mean_peers = self._normalize_()\n",
    "        \n",
    "        # construction of the correlation matrix\n",
    "        self._CorMatrix_(adjust, discount, verbose)\n",
    "\n",
    "    def predict(self, uid, mid, k=4):\n",
    "        '''\n",
    "        :param: uid l'id de l'utilisateur\n",
    "                mid l'id du film\n",
    "                k la taille des peer-groupes\n",
    "        :return: une note prédite pour le couple peer-other (p, o)\n",
    "        '''\n",
    "        assert self.dfr is not None, \"This MemoryBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        \n",
    "        p, o = (uid, mid) if self.base == 'user' else (mid, uid)\n",
    "        \n",
    "        # si peer et/ou other est inconnu, prédire des valeurs arbitraire\n",
    "        peer_unknown = p not in self.mean_peers\n",
    "        other_unknown = o not in self.dfr.index.droplevel(self.ptype)\n",
    "        \n",
    "        if peer_unknown and other_unknown:\n",
    "            return 2.5\n",
    "        elif peer_unknown:\n",
    "            return float(self.dfr.loc[(slice(None), o), 'rating'].mean())\n",
    "        elif other_unknown:\n",
    "            return self.mean_peers[p]\n",
    "        \n",
    "        mu = self.mean_peers[p]\n",
    "        peers = self._PeerGroup_(p, o, k)\n",
    "\n",
    "        # calculer la moyenne pondérée par le taux de corrélation des notes normalisés des amis dans le peer-groupe\n",
    "        sum_up, sum_down = 0, 0\n",
    "        for friend in peers:\n",
    "            cor = self.get_cor(p, friend)\n",
    "            if not math.isnan(cor):\n",
    "                sfo = self.dfr.loc[(friend, o), 'rating_norm_'+self.base]\n",
    "                sum_up += sfo * cor\n",
    "                sum_down += abs(cor)\n",
    "\n",
    "        # valeur par défaut permettant de faire la divison (si sum_down est à 0, sum_up aussi) \n",
    "        sum_down = 1 if sum_down == 0 else sum_down\n",
    "        pred = mu + sum_up / sum_down\n",
    "\n",
    "        # note prédite à une précision de 0.5\n",
    "#         pred = round(2 * pred) / 2\n",
    "        # note prédite se trouve entre 0 et 5\n",
    "        pred = 5 if pred > 5 else pred\n",
    "        pred = 0 if pred < 0 else pred\n",
    "\n",
    "        return pred\n",
    "        \n",
    "    def score(self, test_ratings, k=4):\n",
    "        '''\n",
    "        :param: test_ratings - l'échantillon de test\n",
    "                k - la taille des peer-groupes\n",
    "        :return: un score entre 0 et 1 correspondant à [1 - erreur]\n",
    "        '''\n",
    "        assert self.dfr is not None, \"This MemoryBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        dft = test_ratings.set_index([self.ptype, self.otype])\n",
    "        predictions = [self.predict(p, o, k) for (p, o) in list(dft.index)]\n",
    "        truth = dft['rating'].to_numpy()\n",
    "        \n",
    "        return 1 - math.sqrt(sum((predictions - truth)**2) / len(predictions)) / 5\n",
    "    \n",
    "    \n",
    "    def get_cor(self, u, v):\n",
    "        '''\n",
    "        :param: u, v deux id de peers\n",
    "        :return: le taux de corrélation entre u et v\n",
    "        '''\n",
    "        assert self.cm is not None, \"This MemoryBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        if u in self.id_rank and v in self.id_rank:\n",
    "            ixu, ixv = self.id_rank[u], self.id_rank[v]\n",
    "            return self.cm[ixu, ixv]\n",
    "        return float('nan') # valeur par défaut si pas de corrélation calculée\n",
    "    \n",
    "    def _normalize_(self):\n",
    "        '''\n",
    "        Ajoute une colonne dans la dataframe df contenant les notes normalisées des peers\n",
    "        :return: un dictionnaire de clefs les id des peers et de valeurs leurs notes moyennes\n",
    "        '''\n",
    "        mean = self.dfr.groupby(self.ptype).mean()['rating'].to_dict()\n",
    "        new_col = 'rating_norm_'+self.base\n",
    "        self.dfr[new_col] = self.dfr.apply(lambda row : row[0]- mean[row.name[0]] , axis=1) \n",
    "        return mean\n",
    "    \n",
    "    def _CorMatrix_(self, adjust, discount, verbose=False):\n",
    "        '''\n",
    "        Construit la matrice de corrélation entre peers et le dictionnaire associant les id aux rangs\n",
    "        :param: adjust - un booléen indiquant s'il faut ajuster la corrélation\n",
    "                discount - un entier indiquant le seuil de other en commun à avoir (paramètre beta)\n",
    "                        donner 0 pour indiquer de ne pas utiliser la variante discounted\n",
    "                verbose un booléen indiquant s'il faut affihcer le suivi des calculs\n",
    "        '''\n",
    "        peers = self.dfr.index.get_level_values(self.ptype).unique()\n",
    "        nb_peers = len(peers)\n",
    "\n",
    "        if verbose:\n",
    "            logger = logging.getLogger()\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logging.info('nb of peers: {}'.format(nb_peers))\n",
    "\n",
    "        self.id_rank = {}\n",
    "        self.cm = np.empty((nb_peers,nb_peers))\n",
    "        self.cm[:] = np.nan\n",
    "\n",
    "        for i in range(nb_peers):\n",
    "            u = peers[i]\n",
    "            self.id_rank[u] = i\n",
    "            if verbose and not i % 10 : \n",
    "                logging.info('peer nb: {} (id {})'.format(i, u))\n",
    "            for j in range(i + 1, nb_peers):\n",
    "                v = peers[j]\n",
    "                correlation = self.tx_cor(u, v, adjust, discount)\n",
    "                if not np.isnan(correlation):\n",
    "                    self.cm[i, j] = correlation\n",
    "                    self.cm[j, i] = self.cm[i, j]\n",
    "    \n",
    "    def _PeerGroup_(self, p, o, k=4):\n",
    "        '''\n",
    "        :param: p l'id du peer\n",
    "                o l'id de other\n",
    "                k la taille des peer-groupes\n",
    "        :return: la liste des k peers les plus proches de p\n",
    "        '''\n",
    "        # get peers that rated the other o\n",
    "        peers = self.dfr.loc[(slice(None), o), :]\n",
    "        peers = peers.index.get_level_values(self.ptype).unique()\n",
    "        \n",
    "        top = [(float('-inf'), p)] * k\n",
    "        for v in peers:\n",
    "            taux = self.get_cor(p, v)\n",
    "            if taux > top[-1][0] :\n",
    "                top += [(taux, v)]\n",
    "                top.sort(reverse=True)\n",
    "                top = top[:-1]\n",
    "        return [t[1] for t in top]\n",
    "    \n",
    "\n",
    "    def tx_cor(self, u, v, adjust=False, discount=0):\n",
    "        '''\n",
    "        :param: u, v - les id des peers (user ou movie) à comparer\n",
    "                adjust - un booléen indiquant s'il faut ajuster la corrélation\n",
    "                discount - un entier indiquant le seuil de other en commun à avoir (paramètre beta)\n",
    "                        donner 0 pour indiquer de ne pas utiliser la variante discounted\n",
    "        :return: le taux de corrélation classique entre u et v.\n",
    "        '''\n",
    "        Iu = self.dfr.loc[u, :]\n",
    "        Iv = self.dfr.loc[v, :]\n",
    "        Iuv = list(set(Iu.index) & set(Iv.index))\n",
    "        if not len(Iuv) : # l'intersection est vide\n",
    "            return float('nan')\n",
    "        \n",
    "        su = Iu.loc[Iuv]\n",
    "        sv = Iv.loc[Iuv]\n",
    "        su = su['rating_norm_'+self.base].to_numpy()\n",
    "        sv = sv['rating_norm_'+self.base].to_numpy()\n",
    "\n",
    "        if adjust:\n",
    "            card_Uk = self.dfr.loc[(slice(None), Iuv), :].groupby(self.otype).count()['rating_norm_'+self.base].to_numpy()\n",
    "            up = np.dot(su / card_Uk, sv)\n",
    "            down = math.sqrt(np.dot(su / card_Uk, su) * np.dot(sv / card_Uk, sv))\n",
    "        else:\n",
    "            up = np.dot(su, sv)\n",
    "            down = math.sqrt(np.dot(su, su) * np.dot(sv, sv))\n",
    "        \n",
    "        # default value arbitrarily set to 0\n",
    "        if up == 0 or down == 0:\n",
    "            return 0\n",
    "        \n",
    "        correlation = up / down\n",
    "        \n",
    "        if discount > 0:\n",
    "            correlation *= min(len(Iuv), discount) / discount\n",
    "            \n",
    "        return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based recommendation system <a id='model'></a>\n",
    "\n",
    "[retour au sommaire](#sommaire)\n",
    "\n",
    "[aller au modèle linéaire](#linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idée du modèle\n",
    "\n",
    "La matrice des notes user-item $R$ est partiellement vide. Ainsi réduire les dimensions de la matrice pourrait améliorer la complexité de nos algorithmes. Une méthode que nous pourrions avoir envie d'utiliser est la décomposision en valeurs singulières : $R = U_{svd} \\Sigma V_{svd}$. Cependant cette méthode ne s'applique pas ici étant donné que $R$ n'est pas complète et qu'on a besoin de réaliser des calculs algébriques avec $R$ pour trouver la décomposition.\n",
    "\n",
    "On considère donc un modèle dans lequel il existe des attributs (features) décrivants les films et les préférences des utilisateurs. La matrice $R$ peut alors être factorisée en produit de deux matrices $U$ et $V$ représentant respectivement les utilisateurs et les items :\n",
    "\n",
    "$$\n",
    "R \\approx U \\times V^T\n",
    "$$\n",
    "\n",
    "avec $R \\in \\mathbb{R}^{n \\times m}$ la matrice des notes user-item, $U \\in \\mathbb{R}^{n \\times \\ell}$ la matrice des users, $V \\in \\mathbb{R}^{m \\times \\ell}$ la matrice des items et $\\ell$ le nombre d'attributs. Pour faire un rapprochement avec la SVD, on peut considerer que $U = U_{svd} \\Sigma^{1/2}$ et $V = \\Sigma^{1/2} V_{svd}$. On note $U_i$ les lignes de $U$ et $V_j$ les lignes de $V$ :\n",
    "$\n",
    "U = \\left[ \\begin{array}{c} U_1 \\\\ \\vdots \\\\ U_n \\end{array} \\right]\n",
    "$ et \n",
    "$\n",
    "V = \\left[ \\begin{array}{c} V_1 \\\\ \\vdots \\\\ V_m \\end{array} \\right]\n",
    "$\n",
    "avec $U_i^T, V_j^T \\in \\mathbb{R^\\ell}$.\n",
    "\n",
    "Dans ce modèle, chaque note $R_{ij}$ associée à un couple user-item $(i, j)$ est le résultat du produit scalaire entre la ligne associée au user $i$ dans $U$ et la ligne associée au item $j$ dans $V$ : $R_{ij} = U_i \\cdot V_j^T$. Une fois les matrices $U$ et $V$ apprises, pour prédire une note il suffira de faire le produit scalaire entre les lignes correspondantes.\n",
    "\n",
    "Trouver $U$ et $V$ revient à minimiser l'erreur entre la note prédite $U_i \\cdot V_j^T$ et la véritable note $R_{ij}$. Il s'agit du problème de minimisation suivant, avec $E = \\{(i, j) \\mbox{ | } R_{ij} \\mbox{ connue}\\}$ :\n",
    "\n",
    "$$\n",
    "(U, V) = argmin_{(U, V)} \\sum_{(i, j) \\in E} [U_i \\cdot V_j^T - R_{ij}]^2\n",
    "$$\n",
    "\n",
    "qui est équivalent à:\n",
    "\n",
    "$$\n",
    "(U, V) = argmin_{(U, V)} \\frac{1}{2}\\sum_{(i, j) \\in E} [U_i \\cdot V_j^T - R_{ij}]^2 + \\lambda (\\|U_i\\|^2 + \\|V_j\\|^2)\n",
    "$$\n",
    "\n",
    "Le terme de droite est un terme régulateur, de paramètre $\\lambda$ à ajuster, permettant de prévenir un overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation avec une descente de gradient (à pas constant)\n",
    "\n",
    "<span style='color:blue'> justification du choix d'un SGD et pas regression plus classique : est-ce parce que c'est une somme sur E donc pas d'algèbre précise possible ? </span>\n",
    "\n",
    "Dans notre [cours d'optimisation](https://www.ceremade.dauphine.fr/~gontier/enseignement.html) donné par David Gontier, nous avons étudié différentes méthodes de descente de gradient de complexité et d'optimalité différentes. Cependant il nous semble qu'utiliser une version simple à pas $\\tau$ constant suffit.\n",
    "Notre fonction objective est la suivante :\n",
    "$$\n",
    "F(U, V) := \\sum_{(i, j) \\in E} \\frac{1}{2}[U_i \\cdot V_j^T - R_{ij}]^2 + \\frac{\\lambda}{2} (\\|U_i\\|^2 + \\|V_j\\|^2)\n",
    "$$\n",
    "\n",
    "Dans une descente de gradient classique, à chaque itération on met à jour $U$ et $V$ suivant la formule \n",
    "$\n",
    "(U, V) = (U, V) - \\tau \\nabla F(U, V)\n",
    "$. Cependant, dans notre cas nous n'allons pas mettre à jour toutes les lignes de $U$ et $V$ simultanément. En effet, puisque la somme dans $F$ ne se fait que sur les couples $(i, j)$ pour lesquels la note est connue, nous allons seulement mettre à jour le couple $(U_i, V_j)$ associé en itérant sur tous les couples $(i, j) \\in E$. \n",
    "\n",
    "Pour une note $R_{ij}$, on a \n",
    "$\n",
    "\\frac{\\partial F}{\\partial U_i} = V_j^T (U_i \\cdot V_j^T - R_{ij}) + \\lambda U_i\n",
    "$\n",
    " et \n",
    "$\n",
    "\\frac{\\partial F}{\\partial V_j} = Ui (U_i \\cdot V_j^T - R_{ij}) + \\lambda V_j\n",
    "$\n",
    "donc on peut mettre à jour les lignes $U_i$ et $V_j$ selon les formules \n",
    "$$\n",
    "U_i = Ui - \\tau [V_j^T (U_i \\cdot V_j^T - R_{ij}) + \\lambda U_i]\\\\\n",
    "V_j = V_j - \\tau [Ui (U_i \\cdot V_j^T - R_{ij}) + \\lambda V_j]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détails d'implémentation\n",
    "\n",
    "De même que pour le cas d'un prédicteur memory-based, nous avons fait le choix d'utiliser une matrice numpy pour représenter $R$. Celle-ci est certe essentiellement vide, mais l'accès est plus rapide et nous avons préféré privilégier le temps au coût d'un stockage inutil de vide. Néanmoins ceci nécessite également d'associer un id à un rang. Cette association doit pouvoir être faite dans les deux sens. Pour cela, nous utilisons une liste et un dictionnaire. Pour les user par exemple, `user_id` est une liste qui pour un rang donné permet d'avoir l'id correspondant : `user_id[i] = uid` où `i` est le rang correspondant à l'id `uid`. Pour avoir la correspondance dans l'ordre sens, nous utilisons le dictionnaire `user_rank` où `user_rank[uid] = i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedPredictor:\n",
    "    def __init__(self):\n",
    "        self.R = None\n",
    "        self.E = None\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "        self.user_id = None\n",
    "        self.movie_id = None\n",
    "        self.user_rank = None\n",
    "        self.movie_rank = None\n",
    "    \n",
    "    def fit(self, ratings, clu_fea=None, ell=100, lamb=1.5, tau=0.05, tol=1e-3, Niter=1000, verbose=False):\n",
    "        '''\n",
    "        Apprentissage des paramètres U et V\n",
    "        s\n",
    "        :param: ratings - la dataframe contenant les notes des utilisateurs donnés aux films\n",
    "                ell - le nombre de features du modèle\n",
    "                lamb - le terme de régularisation de la fonction objective\n",
    "                tau - le pas de la descente de gradient (taux d'apprentissage)\n",
    "                tol - la tolérance\n",
    "                Niter - le nombre d'itérations maximal\n",
    "                verbose - un booléen indiquant s'il faut afficher le suivi des calculs\n",
    "        '''\n",
    "        \n",
    "        if self.R is not None: #already fitted\n",
    "            return\n",
    "        \n",
    "        if verbose:\n",
    "            logger = logging.getLogger()\n",
    "            logger.setLevel(logging.INFO)\n",
    "        \n",
    "        dfr = ratings[['userId', 'movieId', 'rating']]\n",
    "\n",
    "        # correspondance rang - id\n",
    "        self.user_id = dfr['userId'].unique().tolist()\n",
    "        self.movie_id = dfr['movieId'].unique().tolist()\n",
    "        self.user_id.sort()\n",
    "        self.movie_id.sort()\n",
    "        # correspondance id - rang\n",
    "        self.user_rank, self.movie_rank = {}, {}\n",
    "        for uid in self.user_id:\n",
    "            self.user_rank[uid] = len(self.user_rank)\n",
    "        for mid in self.movie_id:\n",
    "            self.movie_rank[mid] = len(self.movie_rank)\n",
    "\n",
    "        # transformer la dataframe en matric numpy\n",
    "        dfr.set_index(['userId', 'movieId'], inplace=True)\n",
    "        self.R = dfr.unstack(level='movieId').to_numpy()\n",
    "            \n",
    "        # construction de l'ensemble E, ensemble des rangs pour lesquels R[i, j] est connu\n",
    "        ids = list(dfr.index)\n",
    "        self.E = list(map(lambda t : (self.user_rank[t[0]], self.movie_rank[t[1]]), ids))\n",
    "        \n",
    "        if clu_fea is not None: # linear mode\n",
    "            linear=True\n",
    "            # construction de la matrice V a partir du DataFrame clu_fea\n",
    "            V = select_useful_features(clu_fea)\n",
    "            V = normalize_runtime(V)\n",
    "            V = normalize_release_date(V)\n",
    "            V = normalize_lang(V)\n",
    "            V = normalize_genres(V)\n",
    "            V = normalize_prod_countries(V)\n",
    "            self.V = V.to_numpy()\n",
    "            ell = V.shape[1]\n",
    "        else:\n",
    "            linear=False\n",
    "            # initialisation aléatoire de V\n",
    "            m = len(self.movie_rank)\n",
    "            self.V = np.random.rand(m, ell)\n",
    "        \n",
    "        # initialisation aléatoire de U\n",
    "        n = len(self.user_rank)\n",
    "        self.U = np.random.rand(n, ell)\n",
    "    \n",
    "        # résolution du problème d'optimisation\n",
    "        self._descenteGradient_(lamb, tau, tol, Niter, linear=linear, verbose=verbose)\n",
    "        \n",
    "    def predict(self, uid, mid):\n",
    "        '''\n",
    "        :param: uid - l'id de l'utilisateur\n",
    "                mid - l'id du film\n",
    "        :return: la note prédite pour le couple user-movie (uid, mid)\n",
    "        '''\n",
    "        assert self.E is not None, \"This ModelBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        ell = self.U.shape[1]\n",
    "        \n",
    "        # selection des lignes correspondantes, vecteurs aléatoires si inconnus\n",
    "        u = self.U[self.user_rank[uid]] if uid in self.user_rank else np.random.rand(ell)\n",
    "        v = self.V[self.movie_rank[mid]] if mid in self.movie_rank else np.random.rand(ell)\n",
    "        \n",
    "        #prédiction\n",
    "        pred = np.dot(u, v.T)\n",
    "#         pred = round(2 * pred) / 2 # la note prédite à une précision de 0.5\n",
    "        pred = 5 if pred > 5 else pred # la note prédite se trouve entre 0 et 5\n",
    "        pred = 0 if pred < 0 else pred\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def score(self, test_ratings):\n",
    "        '''\n",
    "        :param: test_ratings l'échantillon de test\n",
    "        :return: un score entre 0 et 1 correspondant à [1 - erreur]\n",
    "        '''\n",
    "        assert self.E is not None, \"This ModelBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        \n",
    "        dft = test_ratings.set_index(['userId', 'movieId'])\n",
    "        predictions = [self.predict(uid, mid) for (uid, mid) in list(dft.index)]\n",
    "        truth = dft['rating'].to_numpy()\n",
    "    \n",
    "        return 1 - math.sqrt(sum((predictions - truth)**2) / len(predictions) ) / 5\n",
    "        \n",
    "\n",
    "    def _F_(self, lamb):\n",
    "        '''\n",
    "        :param: lamb - le terme régulateur\n",
    "        :return: l'évaluation de la fonction objective à minimiser avec les paramètres actuels U et V\n",
    "        '''\n",
    "        return sum([1/2 * (np.dot(self.U[i], self.V[j].T) - self.R[i, j])**2 \\\n",
    "                    + lamb/2 * (sum(self.U[i] ** 2) + sum(self.V[j] ** 2))  for (i, j) in self.E])\n",
    "    \n",
    "    def _F_linear_(self, lamb):\n",
    "        return sum([1/2 * (np.dot(self.U[i], self.V[j]) - self.R[i, j])**2 \\\n",
    "                    + lamb/2 * sum(self.U[i] ** 2) for (i, j) in self.E])\n",
    "    \n",
    "    def _descenteGradient_(self, lamb, tau, tol=1e-3, Niter=1000, linear=False, verbose=False):\n",
    "        '''\n",
    "        Réalise une descente de gradient pour minimiser la fonction objective.\n",
    "    \n",
    "        :param: lamb - le terme de régularisation de la fonction objective\n",
    "                tau - le pas de la descente de gradient (taux d'apprentissage)\n",
    "                tol - la tolérance\n",
    "                Niter - le nombre d'itérations maximal\n",
    "                linear - un booléen indiquant si la fonction objective à minimiser correspond au modèle linéaire\n",
    "                verbose - un booléen indiquant s'il faut afficher le suivi des calculs\n",
    "        '''\n",
    "        if verbose:\n",
    "            logging.info('nombre de couples : {}'.format(len(self.E)))\n",
    "            logging.info(\"nombre d'iteration max : {}\".format(Niter))\n",
    "            \n",
    "        last_F = 0\n",
    "        for n in range(Niter):\n",
    "            \n",
    "            new_F = self._F_linear_(lamb) if linear else self._F_(lamb)\n",
    "\n",
    "            if verbose :\n",
    "                logging.info(\"{}. pente de F: {}\".format(n, abs(new_F - last_F)))\n",
    "\n",
    "            if abs(new_F - last_F) < tol:\n",
    "                return\n",
    "            last_F = new_F   \n",
    "\n",
    "            for (i, j) in self.E:\n",
    "                gradU = self.V[j].T * (np.dot(self.U[i], self.V[j].T) - self.R[i, j]) + lamb * self.U[i]\n",
    "                self.U[i] -= tau * gradU\n",
    "                if not linear:\n",
    "                    gradV = self.U[i] * (np.dot(self.U[i], self.V[j].T) - self.R[i, j]) + lamb * self.V[j]\n",
    "                    self.V[j] -= tau * gradV\n",
    "        \n",
    "        if verbose:     \n",
    "            logging.info(\"Erreur, l’algorithme n’a pas convergé après\", Niter ,\" itérations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model : content-based <a id='linear'></a>\n",
    "\n",
    "[retour au titre de section](#model)\n",
    "\n",
    "On remarque que si $U$ ou $V$ est fixé, la fonction objective devient quadratique, ou presque puisque la somme ne se fait que sur $E$. De plus, on peut construire une matrice d'attributs des films à partir des informations dont on dispose. C'est ce qu'on va faire ci-dessous. Les résultats qui suivent - notemment les fonctions définies - seront utilisés dans la classe ModelBasedPredictor (cellule du dessus) pour l'approche linéaire.<br><br>\n",
    "On va détailler ici la construction de la matrice V à partir des ratings et des films. Le problème principal réside dans les attributs de films qui ne sont pas des nombres. En effet, il est facile de normaliser le runtime et la release_date mais comment faire pour des champs tels que les genres ? La réponse : augmenter le nombre d'attributs des films. Pour chaque film on créé un attribut par genre de film, qu'on remplit par une valeur binaire si le film appartient ou non au genre en question. Enfin, on évite que des attributs comme genres n'écrasent ceux comme runtime en divisant les valeur binaire par le nombre de valeur binaires à 1 d'un film pour l'attribut genres par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_linear = movies.join(link.set_index('tmdbId'), on='tmdbId', how='inner')\n",
    "dfm_linear = dfm_linear.merge(ratings.drop_duplicates('movieId'), how='inner')\n",
    "ratings_linear = ratings.merge(dfm_linear.movieId, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_useful_features(V):\n",
    "    return V[['genres', 'release_date', 'production_countries', 'original_language', 'runtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = select_useful_features(dfm_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_runtime(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    min_runtime = min(dfm.runtime)\n",
    "    max_runtime_diff = max(dfm.runtime) - min_runtime\n",
    "    if max_runtime_diff <= 1:\n",
    "        return dfm\n",
    "    dfm.runtime = dfm.runtime.apply(lambda x: (x - min_runtime) / max_runtime_diff)\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_runtime(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_release_date(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    simplify_date(dfm)\n",
    "    min_date = min(dfm.release_date)\n",
    "    max_date_diff = max(dfm.release_date) - min_date\n",
    "    if max_date_diff <= 1:\n",
    "        return dfm\n",
    "    dfm.release_date = dfm.release_date.apply(lambda x: (x - min_date) / max_date_diff)\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_release_date(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lang(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    if 'original_language' not in dfm.columns:\n",
    "        return dfm\n",
    "    languages = np.unique(dfm.original_language)\n",
    "    nb_lang = len(languages)\n",
    "    for lang in languages:\n",
    "        dfm[lang] = pd.Series(dtype='float64')\n",
    "        dfm[lang] = dfm[lang].fillna(0)\n",
    "    for index, lang in dfm.original_language.iteritems():\n",
    "        dfm.at[index, lang] = 1\n",
    "    dfm = dfm.drop('original_language', axis=1)\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_lang(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_genres(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    if 'genres' not in dfm.columns:\n",
    "        return dfm\n",
    "    vectorize_genres(dfm)\n",
    "    genres_list = np.array([])\n",
    "    for _, tab in dfm.genres.iteritems():\n",
    "        genres_list = np.unique(np.append(genres_list, tab))\n",
    "    for genre_id in genres_list:\n",
    "        col_name = \"genId\" + str(int(genre_id))\n",
    "        dfm[col_name] = pd.Series(dtype='float64')\n",
    "        dfm[col_name] = dfm[col_name].fillna(0)\n",
    "    for index, gen in dfm.genres.iteritems():\n",
    "        for g in gen:\n",
    "            dfm.at[index, \"genId\" + str(g)] = 1 / len(gen)\n",
    "    dfm = dfm.drop('genres', axis=1)\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_genres(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prod_countries(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    if 'production_countries' not in dfm:\n",
    "        return dfm\n",
    "    simplify_countries(dfm)\n",
    "    country_list = np.array([])\n",
    "    for _, tab in dfm.production_countries.iteritems():\n",
    "        country_list = np.unique(np.append(country_list, tab))\n",
    "    for country in country_list:\n",
    "        dfm[country] = pd.Series(dtype='float64')\n",
    "        dfm[country] = dfm[country].fillna(0)\n",
    "    for index, countries in dfm.production_countries.iteritems():\n",
    "        for c in countries:\n",
    "            dfm.at[index, c] = 1 / len(countries)\n",
    "    dfm = dfm.drop('production_countries', axis=1)\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_prod_countries(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>af</th>\n",
       "      <th>ar</th>\n",
       "      <th>bn</th>\n",
       "      <th>bo</th>\n",
       "      <th>bs</th>\n",
       "      <th>cn</th>\n",
       "      <th>cs</th>\n",
       "      <th>da</th>\n",
       "      <th>...</th>\n",
       "      <th>TT</th>\n",
       "      <th>TW</th>\n",
       "      <th>UA</th>\n",
       "      <th>UG</th>\n",
       "      <th>US</th>\n",
       "      <th>UY</th>\n",
       "      <th>VN</th>\n",
       "      <th>XC</th>\n",
       "      <th>ZA</th>\n",
       "      <th>ZW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.091228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.088596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.111404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.092982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9020</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.074561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9021</th>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.074561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9022</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9023</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9024</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.086842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9025 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      release_date   runtime   af   ar   bn   bo   bs   cn   cs   da  ...  \\\n",
       "0         0.815789  0.071053  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1         0.815789  0.091228  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2         0.815789  0.088596  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3         0.815789  0.111404  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4         0.815789  0.092982  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...            ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9020      1.000000  0.074561  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "9021      0.868421  0.074561  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "9022      1.000000  0.131579  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "9023      1.000000  0.135965  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "9024      1.000000  0.086842  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "       TT   TW   UA   UG   US   UY   VN   XC   ZA   ZW  \n",
       "0     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "9020  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9021  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9022  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9023  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9024  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9025 rows x 156 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix des hyperparamètres\n",
    "\n",
    "Dans ce modèle, l'utilisateur doit choisir 3 hyperparamètres : $\\ell$, $\\lambda$ et $\\tau$. Pour choisir les valeurs données par défaut, nous avons fait des tests de manière exhaustive sur un ensemble de valeurs d'hyperparamètre (grid search). Pour chaque triplet de valeurs d'hyperparametre, on calcul le score par validation croisée. Nous n'avons pas eu le temps de ré-exécuter la cellule réalisant ce gridSearchCV pour vous montrer le résulat. Néanmoins nous avons réalisé la recherche sur un échantillon de taille 100 pour vous donner une idée de son fonctionnement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_prop):\n",
    "    '''\n",
    "    Sépare la dataframe df en deux dataframe train et test.\n",
    "    \n",
    "    :param: df - la dataframe à séparer\n",
    "            train_prop - la proportion d'element qu'il doit y avoir dans le train dataframe\n",
    "    :return: les train et test dataframes\n",
    "    '''\n",
    "    train = df.sample(frac=train_prop)\n",
    "    test = df.drop(train.index)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(df, nb_folds):\n",
    "    '''\n",
    "    Sépare la dataframe df en nb_folds dataframe de taille (presque) égales.\n",
    "    \n",
    "    :param: df - la dataframe à séparer\n",
    "            nb_folds - le nombre d'échantillons à consitutuer à partir de df\n",
    "    :return: la liste des dataframes\n",
    "    '''\n",
    "    N = len(df)\n",
    "    f_size = N // nb_folds\n",
    "    folds = [None] * nb_folds\n",
    "    for i in range(nb_folds - 1):\n",
    "        folds[i] = df.sample(n=f_size)\n",
    "        df = df.drop(folds[i].index)\n",
    "    folds[nb_folds - 1] = df\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(predictor, df, params, nb_folds, verbose=False):\n",
    "    '''\n",
    "    :param: predictor - une classe de predicteur qui implémente une méthode fit\n",
    "            df - la datafraeme des données\n",
    "            params - une dictionnaire des paramètres à donner à la méthode fit\n",
    "                    où les clefs sont le nom des paramètred et les valeurs la valeur des paramètres\n",
    "            nb_folds - le nombre de sous-échantillons utilisés dans la validation croisée\n",
    "    :return: le score de validation croisée\n",
    "    '''\n",
    "    # les arguments données dans le dico param doivent correspondrent aux argument de la méthode fit de predictor\n",
    "    var = getattr(getattr(predictor.fit, '__code__'), 'co_varnames')\n",
    "    assert params.keys() <= set(var[1:]), 'invalid arguments given in params' # var[1:] pour enlever 'self'\n",
    "    \n",
    "    folds = k_fold(df, nb_folds)\n",
    "    scores = [None] * nb_folds\n",
    "    \n",
    "    for i in range(nb_folds):\n",
    "        test = folds[i]\n",
    "        train = df.drop(test.index)\n",
    "        \n",
    "        t1 = time()\n",
    "        pred = predictor()\n",
    "        pred.fit(train, **params)\n",
    "        scores[i] = pred.score(test)\n",
    "        t2 = time()\n",
    "        \n",
    "        if verbose:\n",
    "            print('fold {} : RMSE={} (time: {})'.format(i+1, scores[i], t2 - t1))\n",
    "            \n",
    "    cv = sum(scores) / nb_folds\n",
    "    \n",
    "    if verbose:\n",
    "        print('total RMSE={}'.format(cv))\n",
    "        \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinaisons(grid):\n",
    "    '''\n",
    "    :return: un generator de dictionnaires de toutes les combinaisons des valeurs de grid\n",
    "    '''\n",
    "    for v in product(*grid.values()):\n",
    "        yield {id:v[i] for i, id in enumerate(grid.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchCV(predictor, df, param_grid, cv, verbose=False):\n",
    "    '''\n",
    "    :param: predictor - une classe de predicteur qui implémente une méthode fit\n",
    "            df - la datafraeme des données\n",
    "            param_grid - le dictionnaire de la grille des paramètres à tester\n",
    "            cv - le nombre de sous-échantillons à utiliser dans la validation croisée\n",
    "            verbose - un booléan indiquant s'il faut afficher les score de CV calculés\n",
    "    :return: le meilleur score et le dictionnaire de paramètres associé\n",
    "    '''\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    for param in combinaisons(param_grid):\n",
    "        t1 = time()\n",
    "        score = cross_validation(predictor, df, param, cv)\n",
    "        t2 = time()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_param = param\n",
    "        \n",
    "        if verbose:\n",
    "            print('CV SCORE: {} for PARAM: {} (TIME: {})'.format(score, param, t2-t1))\n",
    "\n",
    "    return best_score, best_param\n",
    "\n",
    "def gridSearch(predictor, df, param_grid, verbose=False):\n",
    "    '''\n",
    "    :param: predictor - une classe de predicteur qui implémente une méthode fit\n",
    "            df - la datafraeme des données\n",
    "            param_grid - le dictionnaire de la grille des paramètres à tester\n",
    "            cv - le nombre de sous-échantillons à utiliser dans la validation croisée\n",
    "            verbose - un booléan indiquant s'il faut afficher les score de CV calculés\n",
    "    :return: le meilleur score et le dictionnaire de paramètres associé\n",
    "    '''\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    train, test = split_train_test(df, 0.8)\n",
    "    for param in combinaisons(param_grid):\n",
    "        if verbose:\n",
    "            print('building and fitting the predictor with {} ...'.format(param))\n",
    "        t1 = time()\n",
    "        pred = predictor()\n",
    "        pred.fit(train, **param)\n",
    "        score = pred.score(test)\n",
    "        t2 = time()\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_param = param\n",
    "        \n",
    "        if verbose:\n",
    "            print('score: {} for param: {} (time: {})'.format(score, param, round(t2-t1, 1)))\n",
    "    \n",
    "    if verbose:\n",
    "        print('best_score: {} for param: {}'.format(best_score, best_param))\n",
    "    return best_score, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SCORE: 0.6411068695499317 for PARAM: {'ell': 100, 'lamb': 0.5, 'tau': 0.01} (TIME: 54.294389963150024)\n",
      "CV SCORE: 0.6206706510679596 for PARAM: {'ell': 100, 'lamb': 0.5, 'tau': 0.05} (TIME: 13.808954954147339)\n",
      "CV SCORE: 0.6348978906480662 for PARAM: {'ell': 100, 'lamb': 1.0, 'tau': 0.01} (TIME: 37.374003887176514)\n",
      "CV SCORE: 0.6290850307328651 for PARAM: {'ell': 100, 'lamb': 1.0, 'tau': 0.05} (TIME: 11.135035991668701)\n",
      "CV SCORE: 0.6419819821516466 for PARAM: {'ell': 100, 'lamb': 1.5, 'tau': 0.01} (TIME: 24.82611584663391)\n",
      "CV SCORE: 0.613596860807013 for PARAM: {'ell': 100, 'lamb': 1.5, 'tau': 0.05} (TIME: 6.733615159988403)\n",
      "CV SCORE: 0.6379219710890561 for PARAM: {'ell': 150, 'lamb': 0.5, 'tau': 0.01} (TIME: 68.87822604179382)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:156: RuntimeWarning: overflow encountered in multiply\n",
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:160: RuntimeWarning: invalid value encountered in subtract\n",
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:159: RuntimeWarning: overflow encountered in multiply\n",
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:122: RuntimeWarning: overflow encountered in double_scalars\n",
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:122: RuntimeWarning: overflow encountered in square\n",
      "/Users/anita/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:157: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV SCORE: nan for PARAM: {'ell': 150, 'lamb': 0.5, 'tau': 0.05} (TIME: 102.75391793251038)\n",
      "CV SCORE: 0.6331546471174386 for PARAM: {'ell': 150, 'lamb': 1.0, 'tau': 0.01} (TIME: 41.536526918411255)\n",
      "CV SCORE: nan for PARAM: {'ell': 150, 'lamb': 1.0, 'tau': 0.05} (TIME: 98.521488904953)\n",
      "CV SCORE: 0.6329962873832299 for PARAM: {'ell': 150, 'lamb': 1.5, 'tau': 0.01} (TIME: 35.304856061935425)\n",
      "CV SCORE: nan for PARAM: {'ell': 150, 'lamb': 1.5, 'tau': 0.05} (TIME: 104.97316980361938)\n",
      "CPU times: user 9min 49s, sys: 8.09 s, total: 9min 57s\n",
      "Wall time: 10min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6419819821516466, {'ell': 100, 'lamb': 1.5, 'tau': 0.01})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'ell' : [100, 150],\n",
    "    'lamb' : [0.5, 1.0, 1.5],\n",
    "    'tau' : [0.01, 0.05]}\n",
    "%time gridSearchCV(ModelBasedPredictor, ratings.sample(100), param_grid, cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compaisons des différents prédicteurs <a id='score'></a>\n",
    "\n",
    "[retour au sommaire](#sommaire)\n",
    "\n",
    "\n",
    "Idéalement, on voulait utiliser des validation-croisée pour comparer les performances des différents prédicteurs. Cependant cette méthode d'évaluation est plus longue qu'un simple score et nos méthodes prennent déjà sufissamment de temps pour tourner que nous avons seulement comparés les scores simple des preducteurs. Nous n'avons pas pu faire des tests exhaustifs, leur exécution étant bien trop longue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_grid = {'base':['user', 'movie'], 'adjust':[True, False], 'discount':[30, 0], 'verbose':[True]}\n",
    "\n",
    "# bs, bp = gridSearch(MemoryBasedPredictor, ratings, param_grid, verbose=True)\n",
    "# print('BEST', bs, bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 4 premiers résultats de la cellule ci-dessus sont les suivants. Les taux de corrélation ajustés sont beaucoup plus long. Ceci vient confirmer notre intuition qu'utiliser un produit matriciel dans le taux classique est bien plus efficace.  \n",
    "\n",
    "* score: 0.8096010626657906 for param: {'base': 'user', 'adjust': True, 'discount': 30, 'verbose': True} (time: 3248.1)\n",
    "* score: 0.8048390127464429 for param: {'base': 'user', 'adjust': True, 'discount': 0, 'verbose': True} (time: 2865.1)\n",
    "* score: 0.8116259772919059 for param: {'base': 'user', 'adjust': False, 'discount': 30, 'verbose': True} (time: 1227.6)\n",
    "* score: 0.8061856069288036 for param: {'base': 'user', 'adjust': False, 'discount': 0, 'verbose': True} (time: 1233.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(ratings, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 15s, sys: 6.02 s, total: 14min 21s\n",
      "Wall time: 14min 33s\n",
      "0.8099371330087541\n"
     ]
    }
   ],
   "source": [
    "user_based = MemoryBasedPredictor()\n",
    "%time user_based.fit(train, base='user', adjust=False, discount=30)\n",
    "print(user_based.score(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:nb of peers: 8386\n",
      "INFO:root:peer nb: 0 (id 1)\n",
      "INFO:root:peer nb: 10 (id 11)\n",
      "INFO:root:peer nb: 20 (id 21)\n",
      "INFO:root:peer nb: 30 (id 31)\n",
      "INFO:root:peer nb: 40 (id 43)\n",
      "INFO:root:peer nb: 50 (id 54)\n",
      "INFO:root:peer nb: 60 (id 65)\n",
      "INFO:root:peer nb: 70 (id 77)\n",
      "INFO:root:peer nb: 80 (id 87)\n",
      "INFO:root:peer nb: 90 (id 101)\n",
      "INFO:root:peer nb: 100 (id 113)\n",
      "INFO:root:peer nb: 110 (id 126)\n",
      "INFO:root:peer nb: 120 (id 145)\n",
      "INFO:root:peer nb: 130 (id 156)\n",
      "INFO:root:peer nb: 140 (id 166)\n",
      "INFO:root:peer nb: 150 (id 177)\n",
      "INFO:root:peer nb: 160 (id 188)\n",
      "INFO:root:peer nb: 170 (id 201)\n",
      "INFO:root:peer nb: 180 (id 213)\n",
      "INFO:root:peer nb: 190 (id 224)\n",
      "INFO:root:peer nb: 200 (id 235)\n",
      "INFO:root:peer nb: 210 (id 247)\n",
      "INFO:root:peer nb: 220 (id 257)\n",
      "INFO:root:peer nb: 230 (id 268)\n",
      "INFO:root:peer nb: 240 (id 278)\n",
      "INFO:root:peer nb: 250 (id 291)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-692c700088b8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, ratings, base, adjust, discount, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# construction of the correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CorMatrix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-692c700088b8>\u001b[0m in \u001b[0;36m_CorMatrix_\u001b[0;34m(self, adjust, discount, verbose)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_peers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mcorrelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtx_cor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-692c700088b8>\u001b[0m in \u001b[0;36mtx_cor\u001b[0;34m(self, u, v, adjust, discount)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mle\u001b[0m \u001b[0mtaux\u001b[0m \u001b[0mde\u001b[0m \u001b[0mcorrélation\u001b[0m \u001b[0mclassique\u001b[0m \u001b[0mentre\u001b[0m \u001b[0mu\u001b[0m \u001b[0met\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         '''\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mIu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mIv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mIuv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1759\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;31m# we may have a nested tuples indexer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m             \u001b[0mcurrent_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1962\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no slices here, handle elsewhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3533\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3535\u001b[0;31m             \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3537\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc_level\u001b[0;34m(self, key, level, drop_level)\u001b[0m\n\u001b[1;32m   2844\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   2938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevel_codes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2940\u001b[0;31m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevel_codes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2941\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2942\u001b[0m                 \u001b[0;31m# The label is present in self.levels[level] but unused:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-07323281ca68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mitem_based\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemoryBasedPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"item_based.fit(train, base='movie', adjust=False, discount=30, verbose=True)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_based\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-692c700088b8>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, test_ratings, k)\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                 \u001b[0mCall\u001b[0m \u001b[0;34m'fit'\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mappropriate\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mdft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-692c700088b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                 \u001b[0mCall\u001b[0m \u001b[0;34m'fit'\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mappropriate\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mdft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-692c700088b8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, uid, mid, k)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# si peer et/ou other est inconnu, prédire des valeurs arbitraire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mpeer_unknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_peers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mother_unknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdroplevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpeer_unknown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOverflowError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "item_based = MemoryBasedPredictor()\n",
    "%time item_based.fit(train, base='movie', adjust=False, discount=30, verbose=True)\n",
    "print(item_based.score(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La construction d'un prédicteur item-based prends énormément de temps (ces 250 itérations sont calculées en 2h). Cette grande différence de temps peut se comprendre par la différence de taille des peers. Il y a beaucoup moins de users que de items dans nos données donc moins de taux de corrélation à calculer dans le cas user-based. Or le gros du travaille est de calculer la matrice de corrélation.\n",
    "\n",
    "Nous aurions aimé pouvoir faire une comparaison plus concrète, mais le temps de construction du user-based est raisonnable. Le score de 0.8 nous semble plutôt faible, mais cela reste tout de même le meilleur score que nous avons trouvé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based\n",
    "\n",
    "Nous allons comparer un predicteur suivant le modèle linéaire et un predicteur suivant le modèle plus globale. Nous nous attendions à une bonne estimation par le modèle global, et à un biais des données de la part du modèle linéaire. En effet ce dernier fonctionne avec un choix de features donné par l'utilisateur et celui-ci a été fait de manière arbitraire. Nous étions très étonnés d'obtenir un score identique, assez bas, de 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:nombre de couples : 80003\n",
      "INFO:root:nombre d'iteration max : 1000\n",
      "INFO:root:0. pente de F: 3293086.0392620913\n",
      "INFO:root:1. pente de F: 3019720.452204372\n",
      "INFO:root:2. pente de F: 5948.149721155525\n",
      "INFO:root:3. pente de F: 325.16962175944354\n",
      "INFO:root:4. pente de F: 24.69568465318298\n",
      "INFO:root:5. pente de F: 2.2370277303853072\n",
      "INFO:root:6. pente de F: 0.23218466236721724\n",
      "INFO:root:7. pente de F: 0.026542069448623806\n",
      "INFO:root:8. pente de F: 0.0030783909023739398\n",
      "INFO:root:9. pente de F: 0.0002814258332364261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 43s, sys: 589 ms, total: 1min 44s\n",
      "Wall time: 1min 44s\n",
      "0.6137993463170583\n"
     ]
    }
   ],
   "source": [
    "linear = ModelBasedPredictor()\n",
    "%time linear.fit(train, clu_fea=dfm_cluster, verbose=True)\n",
    "print(linear.score(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme a convergé très rapidement. En comparant avec la trace du model-based ci-dessous, on voit que l'initialisation de U et la construction de V sont bien plus proches du point minima que dans le cas d'une initialisation aléatoire pour les deux (la pente est plus petite au début de l'algorithme). De plus, puisqu'il n'y a qu'un seul paramètre U à mettre à jour, la descente de gradient se fait bien plus vite. Cependant, le score obtenu 0.6 n'est pas très satisfaisant. Cela peut s'expliquer par le fait que les features dans V ont été imposés, donnant moins de flexibilité au système. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:nombre de couples : 80003\n",
      "INFO:root:nombre d'iteration max : 1000\n",
      "INFO:root:0. pente de F: 22422366.844683796\n",
      "INFO:root:1. pente de F: 21803742.746404514\n",
      "INFO:root:2. pente de F: 112956.31815873802\n",
      "INFO:root:3. pente de F: 46760.1377951632\n",
      "INFO:root:4. pente de F: 26148.711063461902\n",
      "INFO:root:5. pente de F: 16639.00284454308\n",
      "INFO:root:6. pente de F: 11403.143844736042\n",
      "INFO:root:7. pente de F: 8190.062239772757\n",
      "INFO:root:8. pente de F: 6056.28010617994\n",
      "INFO:root:9. pente de F: 4551.562757369422\n",
      "INFO:root:10. pente de F: 3445.966728664469\n",
      "INFO:root:11. pente de F: 2614.8927180457977\n",
      "INFO:root:12. pente de F: 1984.6796288269106\n",
      "INFO:root:13. pente de F: 1506.6557167483843\n",
      "INFO:root:14. pente de F: 1145.3431092351675\n",
      "INFO:root:15. pente de F: 873.3698215663317\n",
      "INFO:root:16. pente de F: 669.2408222023514\n",
      "INFO:root:17. pente de F: 516.1630336137605\n",
      "INFO:root:18. pente de F: 401.2023685586755\n",
      "INFO:root:19. pente de F: 314.5511542339809\n",
      "INFO:root:20. pente de F: 248.87208854604978\n",
      "INFO:root:21. pente de F: 198.7316635887837\n",
      "INFO:root:22. pente de F: 160.13323606259655\n",
      "INFO:root:23. pente de F: 130.14751019742107\n",
      "INFO:root:24. pente de F: 106.62915595987579\n",
      "INFO:root:25. pente de F: 88.00446540082339\n",
      "INFO:root:26. pente de F: 73.11484860104974\n",
      "INFO:root:27. pente de F: 61.10278366616694\n",
      "INFO:root:28. pente de F: 51.32930630736519\n",
      "INFO:root:29. pente de F: 43.314559597638436\n",
      "INFO:root:30. pente de F: 36.69502537231892\n",
      "INFO:root:31. pente de F: 31.1927409801865\n",
      "INFO:root:32. pente de F: 26.59309405327076\n",
      "INFO:root:33. pente de F: 22.72874707041774\n",
      "INFO:root:34. pente de F: 19.46794345631497\n",
      "INFO:root:35. pente de F: 16.705950773437507\n",
      "INFO:root:36. pente de F: 14.358756928471848\n",
      "INFO:root:37. pente de F: 12.358391034882516\n",
      "INFO:root:38. pente de F: 10.64942136616446\n",
      "INFO:root:39. pente de F: 9.186311624478549\n",
      "INFO:root:40. pente de F: 7.931406567280646\n",
      "INFO:root:41. pente de F: 6.853383071778808\n",
      "INFO:root:42. pente de F: 5.926047547371127\n",
      "INFO:root:43. pente de F: 5.127393712115008\n",
      "INFO:root:44. pente de F: 4.438857298460789\n",
      "INFO:root:45. pente de F: 3.844721564790234\n",
      "INFO:root:46. pente de F: 3.33163887943374\n",
      "INFO:root:47. pente de F: 2.8882428216165863\n",
      "INFO:root:48. pente de F: 2.5048312231665477\n",
      "INFO:root:49. pente de F: 2.173105571360793\n",
      "INFO:root:50. pente de F: 1.885955254372675\n",
      "INFO:root:51. pente de F: 1.6372781851678155\n",
      "INFO:root:52. pente de F: 1.421830689883791\n",
      "INFO:root:53. pente de F: 1.235101497382857\n",
      "INFO:root:54. pente de F: 1.0732054370455444\n",
      "INFO:root:55. pente de F: 0.9327936000772752\n",
      "INFO:root:56. pente de F: 0.8109770445735194\n",
      "INFO:root:57. pente de F: 0.705261814116966\n",
      "INFO:root:58. pente de F: 0.6134936812450178\n",
      "INFO:root:59. pente de F: 0.5338108449359424\n",
      "INFO:root:60. pente de F: 0.464603366330266\n",
      "INFO:root:61. pente de F: 0.404478530457709\n",
      "INFO:root:62. pente de F: 0.35223107237834483\n",
      "INFO:root:63. pente de F: 0.30681750224903226\n",
      "INFO:root:64. pente de F: 0.26733407576102763\n",
      "INFO:root:65. pente de F: 0.2329979381756857\n",
      "INFO:root:66. pente de F: 0.2031306148855947\n",
      "INFO:root:67. pente de F: 0.17714400094700977\n",
      "INFO:root:68. pente de F: 0.15452822536462918\n",
      "INFO:root:69. pente de F: 0.13484102324582636\n",
      "INFO:root:70. pente de F: 0.11769879522034898\n",
      "INFO:root:71. pente de F: 0.1027686569141224\n",
      "INFO:root:72. pente de F: 0.08976175409043208\n",
      "INFO:root:73. pente de F: 0.07842727936804295\n",
      "INFO:root:74. pente de F: 0.0685475102509372\n",
      "INFO:root:75. pente de F: 0.05993334809318185\n",
      "INFO:root:76. pente de F: 0.05242054717382416\n",
      "INFO:root:77. pente de F: 0.045866381959058344\n",
      "INFO:root:78. pente de F: 0.04014684574212879\n",
      "INFO:root:79. pente de F: 0.035154139797668904\n",
      "INFO:root:80. pente de F: 0.030794556019827724\n",
      "INFO:root:81. pente de F: 0.026986601355019957\n",
      "INFO:root:82. pente de F: 0.02365939732408151\n",
      "INFO:root:83. pente de F: 0.02075128728756681\n",
      "INFO:root:84. pente de F: 0.018208605819381773\n",
      "INFO:root:85. pente de F: 0.015984681609552354\n",
      "INFO:root:86. pente de F: 0.014038835593964905\n",
      "INFO:root:87. pente de F: 0.012335686828009784\n",
      "INFO:root:88. pente de F: 0.010844401724170893\n",
      "INFO:root:89. pente de F: 0.009538134152535349\n",
      "INFO:root:90. pente de F: 0.008393449010327458\n",
      "INFO:root:91. pente de F: 0.0073899864219129086\n",
      "INFO:root:92. pente de F: 0.00650993874296546\n",
      "INFO:root:93. pente de F: 0.005737814586609602\n",
      "INFO:root:94. pente de F: 0.005060065072029829\n",
      "INFO:root:95. pente de F: 0.004464916535653174\n",
      "INFO:root:96. pente de F: 0.003942029143217951\n",
      "INFO:root:97. pente de F: 0.0034824468311853707\n",
      "INFO:root:98. pente de F: 0.003078289912082255\n",
      "INFO:root:99. pente de F: 0.002722719800658524\n",
      "INFO:root:100. pente de F: 0.002409717417322099\n",
      "INFO:root:101. pente de F: 0.002134062349796295\n",
      "INFO:root:102. pente de F: 0.001891153515316546\n",
      "INFO:root:103. pente de F: 0.0016770150978118181\n",
      "INFO:root:104. pente de F: 0.0014881030074320734\n",
      "INFO:root:105. pente de F: 0.0013213808997534215\n",
      "INFO:root:106. pente de F: 0.0011741281487047672\n",
      "INFO:root:107. pente de F: 0.0010440305341035128\n",
      "INFO:root:108. pente de F: 0.0009289986919611692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 20s, sys: 50.2 s, total: 36min 10s\n",
      "Wall time: 41min 4s\n",
      "0.6455465474051799\n"
     ]
    }
   ],
   "source": [
    "model = ModelBasedPredictor()\n",
    "%time model.fit(train, verbose=True)\n",
    "print(model.score(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthodes hybrides <a id='hybride'></a>\n",
    "\n",
    "[retour au sommaire](#sommaire)\n",
    "\n",
    "Les méthodes de type **Predictor** permettent de prédire une note, mais nous aimerions pouvoir recommander des films à un utilisateurs qu'il est susceptible d'aimer. Pour cela il faudrait prédire la note qu'il donnerait à tous les films qu'il n'a pas encore noté et prélever ceux dont la note prédite est la plus élevée. Ceci serait beaucoup trop coûteux. C'est pourquoi nous avons construi une méthode hybride. Ainsi en utilisant un **Recommender** on ne considère qu'un sous-échantillons de films pour lesquels on prédit une note. \n",
    "\n",
    "Nous avons écrit une première méthode `HybrideRecommender` qui entraine un **Recommender** et un **Predictor** avant de faire les prédictions sur les films ciblés. Nous avons également écrit une méthode `Hybrider` qui prends en entrée un **Recommender** et un **Predictor** tous les deux déjà entrainés et qui se contente de réaliser les prédictions sur les films ciblés. En réalité cette deuxième classe ne fait qu'utiliser la méthode `recommend()` de la première, sans initialiser et fitter des objets. Nous avons donc utilisé un héritage de class pour ne pas dupliquer le code de `recommend()`. La méthode `Hybrider` présente l'avantage de ne pas réentrainer des systèmes déjà utilisés auparavant, tandis que `HybrideRecommender` peut s'utiliser de manière indépendante.\n",
    "\n",
    "Utiliser un PopularityRecommender présente néanmoins un désavantage. Premièrement, seuls les films les plus populaires sont considérés, leur donnant plus de visibilté parmis les utilisateurs. Ainsi un nouveau film qui n'aura pas beaucoup été vu aura que peut de chance d'être recommandé par assez populaire. Le deuxième problème est que cette méthode regroupe les films par genres. Or ce qui caractérise un film est plus vaste que seulement la case dans laquelle il s'inscrit et peut dépendre du réalisateur, du lieu de tournage ou de pleins d'autres facteurs. C'est ici que le clustering nous vient en aide. Cela permet de regrouper les films selon leurs similitudes issues de méta-informations et ainsi d'affiner la recherche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    '''\n",
    "    differents prédicteurs : MemoryBasedPredictor et ModelBasedPredictor\n",
    "    différents recommendeurs : PopularityRecommender et HierarchicalClustering\n",
    "    '''\n",
    "    def __init__(self, predictor, recommender):\n",
    "        \n",
    "        pred = predictor()\n",
    "        rec = recommender()\n",
    "        \n",
    "        self.predictor = pred\n",
    "        self.recommender = rec\n",
    "    \n",
    "    \n",
    "    def fit(self, param_pred, param_rec):\n",
    "        # verification que les arguments donnés dans le dictionnaire params correspondent bien aux paramètre de predictor\n",
    "        var_pred = getattr(getattr(self.predictor.fit, '__code__'), 'co_varnames')\n",
    "        assert set(param_pred.keys()) <= set(var_pred[1:]), 'invalid arguments given in params' # var[1:] pour enlever 'self'\n",
    "        var_rec = getattr(getattr(self.recommender.fit, '__code__'), 'co_varnames')\n",
    "        assert set(param_rec.keys()) <= set(var_rec[1:]), 'invalid arguments given in params' # var[1:] pour enlever 'self'\n",
    "\n",
    "        self.predictor.fit(**param_pred)\n",
    "        self.recommender.fit(**param_rec)\n",
    "    \n",
    "    \n",
    "    def recommend(self, uid, dfr, dfm, nb_reco=20):\n",
    "        lid_filtered = self.recommender.recommend(uid, dfr, nb_reco)\n",
    "        \n",
    "        predictions = pd.DataFrame(columns=['movieId', 'predict_rating'])\n",
    "        for mid in lid_filtered:\n",
    "            rat = self.predictor.predict(uid, mid)\n",
    "            predictions = predictions.append({'movieId':int(mid), 'predict_rating':rat}, ignore_index=True)\n",
    "        \n",
    "        predictions.movieId = predictions.movieId.astype(int)\n",
    "        predictions.set_index('movieId', inplace=True)\n",
    "        reco = dfm.merge(predictions, left_on='movieId', right_on='movieId')\n",
    "        reco.sort_values(by='predict_rating', ascending=False, inplace=True)\n",
    "        reco = reco.head(nb_reco) if len(reco) > nb_reco else reco\n",
    "        \n",
    "        return reco, test_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrider(HybridRecommender):\n",
    "    def __init__(self, pred, rec):\n",
    "        self.predictor = pred\n",
    "        self.recommender = rec\n",
    "    \n",
    "    def fit(self, param_pred, param_rec):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS\n",
    "\n",
    "Nous utilisons le prédicteur linéaire se construisant rapidement pour comparer des méthodes hybrides avec plusieurs recommender différents.\n",
    "\n",
    "Voici des cellules qui nous auraient permis de comparer les scores de différentes méthodes hybrides mais que nous n'avons pas pu éxécuter dans ce notebook. Le temps de construction du dendogramme met 2h, mais la coupe est instantée dans le deuxième fit avec la méthode set_n_clusters_perso est instantanée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = test.sample(1)[['rating']]\n",
    "test_best = dfr.loc[dfr['userId'] == uid].sort_values(by='ratings')\n",
    "test_best = test_best.head(nb_reco) if len(test_best) > 0 else nb_reco\n",
    "test_best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = PopularityRecommender()\n",
    "%time pop.fit(movies, link, ratings)\n",
    "hybrid_pop = Hybrider(linear, pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_pop.recommend(uid, dfr=ratings, dfm=dfm_cluster, nb_reco=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = HierarchicalClusterRecommender()\n",
    "%time cluster.fit(dfm_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_clu = Hybrider(linear, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_clu.recommend(uid, dfr=ratings, dfm=dfm_cluster, nb_reco=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cluster.set_n_clusters_perso(n=20)\n",
    "hybrid_clu_perso = Hybrider(linear, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_clu_perso.recommend(uid, dfr=ratings, dfm=dfm_cluster, nb_reco=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autres hybrides que nous n'avons pas eu le temps de faire tourner :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_test = ratings\n",
    "dfc_test = dfm_cluster\n",
    "\n",
    "uid1 = int(dfr_test.iloc[0]['userId'])\n",
    "uid2 = int(dfr_test.iloc[1]['userId'])\n",
    "\n",
    "dfm = movies[['tmdbId', 'title']]\n",
    "dfm = link.merge(dfm, left_on='tmdbId', right_on='tmdbId').drop(columns=['tmdbId', 'imdbId'])\n",
    "dfm.set_index('movieId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfr_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-based + Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr = HybridRecommender(MemoryBasedPredictor, HierarchicalClusterRecommender)\n",
    "%time hr.fit({'base':'user','ratings':dfr_test},{'dfm_cluster':dfc_test,'method':'set_n_clusters', 'n':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid1, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid2, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-based + Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HybridRecommender(MemoryBasedPredictor, PopularityRecommender)\n",
    "%time hr.fit({'base':'user','ratings':dfr_test}, {'movies':movies,'link':link, 'ratings':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid1, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid2, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based + Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HybridRecommender(ModelBasedPredictor, PopularityRecommender)\n",
    "%time hr.fit({'ratings':dfr_test}, {'movies':movies,'link':link, 'ratings':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid1, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid2, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based + Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HybridRecommender(ModelBasedPredictor, HierarchicalClusterRecommender)\n",
    "%time hr.fit({'ratings':dfr_test}, {'dfm_cluster':dfc_test,'method':'set_n_clusters', 'n':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid1, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid2, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-based + Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HybridRecommender(ModelBasedPredictor, HierarchicalClusterRecommender)\n",
    "%time hr.fit({'ratings':dfr_test, 'clu_fea':dfm_cluster}, {'dfm_cluster':dfc_test,'method':'set_n_clusters', 'n':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid1, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid2, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-based + Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = HybridRecommender(ModelBasedPredictor, PopularityRecommender)\n",
    "%time hr.fit({'ratings':dfr_test, 'clu_fea':dfm_cluster}, {'movies':movies,'link':link, 'ratings':ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid1, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hr.recommend(uid2, dfr=ratings, dfm=dfm, nb_reco=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
